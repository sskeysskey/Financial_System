import json
import sqlite3
import os
import datetime

USER_HOME = os.path.expanduser("~")
BASE_CODING_DIR = os.path.join(USER_HOME, "Coding")

# --- 1. é…ç½®æ–‡ä»¶å’Œè·¯å¾„ ---
BASE_PATH = USER_HOME

# ================= é…ç½®åŒºåŸŸ =================
# å¦‚æœä¸ºç©ºï¼Œåˆ™è¿è¡Œâ€œä»Šå¤©â€æ¨¡å¼ï¼›å¦‚æœå¡«å…¥æ—¥æœŸï¼ˆå¦‚ "2024-11-03"ï¼‰ï¼Œåˆ™è¿è¡Œå›æµ‹æ¨¡å¼
SYMBOL_TO_TRACE = "" 
TARGET_DATE = ""

# SYMBOL_TO_TRACE = "IESC"
# TARGET_DATE = "2026-01-09"

# 3. æ—¥å¿—è·¯å¾„
LOG_FILE_PATH = os.path.join(BASE_PATH, "Downloads", "PE_Volume_trace_log.txt")

PATHS = {
    "config_dir": os.path.join(BASE_CODING_DIR, 'Financial_System', 'Modules'),
    "db_dir": os.path.join(BASE_CODING_DIR, 'Database'),
    "sectors_json": lambda config_dir: os.path.join(config_dir, 'Sectors_All.json'),
    "panel_json": lambda config_dir: os.path.join(config_dir, 'Sectors_panel.json'),
    "description_json": lambda config_dir: os.path.join(config_dir, 'description.json'),
    "tags_setting_json": lambda config_dir: os.path.join(config_dir, 'tags_filter.json'),
    "earnings_history_json": lambda config_dir: os.path.join(config_dir, 'Earning_History.json'),
    "db_file": lambda db_dir: os.path.join(db_dir, 'Finance.db'),
}

# åŠ¨æ€ç”Ÿæˆå®Œæ•´è·¯å¾„
CONFIG_DIR = PATHS["config_dir"]
DB_DIR = PATHS["db_dir"]
DB_FILE = PATHS["db_file"](DB_DIR)
SECTORS_JSON_FILE = PATHS["sectors_json"](CONFIG_DIR)
PANEL_JSON_FILE = PATHS["panel_json"](CONFIG_DIR)
DESCRIPTION_JSON_FILE = PATHS["description_json"](CONFIG_DIR)
TAGS_SETTING_JSON_FILE = PATHS["tags_setting_json"](CONFIG_DIR)
EARNING_HISTORY_JSON_FILE = PATHS["earnings_history_json"](CONFIG_DIR)

CONFIG = {
    "TARGET_SECTORS": {
        "Basic_Materials", "Communication_Services", "Consumer_Cyclical",
        "Consumer_Defensive", "Energy", "Financial_Services", "Healthcare",
        "Industrials", "Real_Estate", "Technology", "Utilities"
    },
    # ========== ç›®æ ‡åˆ†ç»„ (ä¸¤ä¸ªç­–ç•¥å…±ç”¨) ==========
    "TARGET_GROUPS": [
        "OverSell_W", "PE_Deeper", "PE_Deep", 
        "PE_W", "PE_valid", "PE_invalid", "season", "no_season"
    ],
    # ========== ç­–ç•¥1 (PE_Volumeæ”¾é‡ä¸‹è·Œ) å‚æ•° ==========
    "COND8_VOLUME_LOOKBACK_MONTHS": 2,   # è¿‡å» N ä¸ªæœˆ
    "COND8_VOLUME_RANK_THRESHOLD": 4,    # æˆäº¤é‡æ’åå‰ N å (é»˜è®¤3ï¼Œä»£ç é€»è¾‘æ˜¯ <4)
    
    # ========== ç­–ç•¥2 (PE_Volume_upæ´»è·ƒä¸Šæ¶¨) å‚æ•° ==========
    "COND_UP_HISTORY_LOOKBACK_DAYS": 5,  # å†å²è®°å½•å›æº¯å¤©æ•°
    "COND_UP_VOL_RANK_MONTHS": 2,        # æ”¾é‡æ£€æŸ¥å›æº¯æœˆä»½
    "COND_UP_VOL_RANK_THRESHOLD": 4,     # æ”¾é‡æ£€æŸ¥å‰ N å
}

# --- 2. è¾…åŠ©ä¸æ–‡ä»¶æ“ä½œæ¨¡å— ---

def load_tag_settings(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            settings = json.load(f)
        tag_blacklist = set(settings.get('BLACKLIST_TAGS', []))
        hot_tags = set(settings.get('HOT_TAGS', []))
        return tag_blacklist, hot_tags
    except Exception:
        return set(), set()

def load_all_symbols(json_path, target_sectors):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            all_sectors_data = json.load(f)
        symbol_to_sector_map = {}
        for sector, symbols in all_sectors_data.items():
            if sector in target_sectors:
                for symbol in symbols:
                    symbol_to_sector_map[symbol] = sector
        return symbol_to_sector_map
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½symbolså¤±è´¥: {e}")
        return None

def load_symbol_tags(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        symbol_tag_map = {}
        stock_list = data.get('stocks', [])
        for item in stock_list:
            symbol = item.get('symbol')
            tags = item.get('tag', [])
            if symbol:
                symbol_tag_map[symbol] = tags
        return symbol_tag_map
    except Exception:
        return {}

def update_panel_with_conflict_check(json_path, pe_vol_list, pe_vol_notes, pe_vol_up_list, pe_vol_up_notes, log_detail):
    """
    ä¸“é—¨ç”¨äº PE_Volume å’Œ PE_Volume_up çš„å†™å…¥ã€‚
    åŠŸèƒ½ï¼š
    1. å†™å…¥ PE_Volume, PE_Volume_backup, PE_Volume_up, PE_Volume_up_backupã€‚
    2. æ£€æŸ¥è¿™äº› symbol æ˜¯å¦å­˜åœ¨äºæŒ‡å®šçš„ backup åˆ†ç»„ä¸­ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤ã€‚
    """
    # å®šä¹‰éœ€è¦æ£€æŸ¥å¹¶åˆ é™¤ symbol çš„å†²çªåˆ†ç»„
    CONFLICT_GROUPS = [
        "PE_Deep_backup", 
        "PE_Deeper_backup", 
        "PE_W_backup", 
        "OverSell_W_backup", 
        "PE_valid_backup", 
        "PE_invalid_backup", 
        "Strategy12_backup", 
        "Strategy34_backup"
    ]

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {}

    # 1. æ±‡æ€»æ‰€æœ‰å³å°†å†™å…¥ Volume ç³»åˆ—çš„æ–° symbol
    all_new_volume_symbols = set(pe_vol_list) | set(pe_vol_up_list)
    
    if not all_new_volume_symbols:
        log_detail("æ²¡æœ‰æ–°çš„ Volume symbol éœ€è¦å†™å…¥ï¼Œè·³è¿‡å†²çªæ£€æŸ¥ã€‚")
    else:
        log_detail(f"æ­£åœ¨æ£€æŸ¥ {len(all_new_volume_symbols)} ä¸ªæ–° symbol æ˜¯å¦å­˜åœ¨äºæ—§ backup åˆ†ç»„ä¸­...")

        # 2. éå†å†²çªåˆ†ç»„è¿›è¡Œæ¸…ç†
        for group_name in CONFLICT_GROUPS:
            if group_name in data and isinstance(data[group_name], dict):
                original_keys = list(data[group_name].keys())
                # æ‰¾å‡ºäº¤é›† (æ—¢åœ¨æ—§åˆ†ç»„ï¼Œåˆæ˜¯æ–° Volume symbol)
                intersection = set(original_keys) & all_new_volume_symbols
                
                if intersection:
                    # é‡å»ºè¯¥åˆ†ç»„ï¼Œæ’é™¤æ‰äº¤é›†ä¸­çš„ symbol
                    new_group_data = {
                        k: v for k, v in data[group_name].items() 
                        if k not in all_new_volume_symbols
                    }
                    data[group_name] = new_group_data
                    log_detail(f"  -> ä» '{group_name}' ä¸­ç§»é™¤äº†: {sorted(list(intersection))}")

    # 3. å†™å…¥æ–°çš„ Volume åˆ†ç»„æ•°æ®
    # è¾…åŠ©å‡½æ•°ï¼šæ„å»ºå¸¦å¤‡æ³¨çš„å­—å…¸
    def build_group_dict(symbols, notes):
        return {sym: notes.get(sym, "") for sym in sorted(symbols)}

    data['PE_Volume'] = build_group_dict(pe_vol_list, pe_vol_notes)
    data['PE_Volume_backup'] = build_group_dict(pe_vol_list, pe_vol_notes)
    
    data['PE_Volume_up'] = build_group_dict(pe_vol_up_list, pe_vol_up_notes)
    data['PE_Volume_up_backup'] = build_group_dict(pe_vol_up_list, pe_vol_up_notes)

    # 4. ä¿å­˜æ–‡ä»¶
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        log_detail("Panel æ–‡ä»¶æ›´æ–°å®Œæˆ (åŒ…å«å†²çªæ¸…ç†)ã€‚")
    except Exception as e:
        log_detail(f"é”™è¯¯: å†™å…¥ Panel JSON æ–‡ä»¶å¤±è´¥: {e}")

def update_earning_history_json(file_path, group_name, symbols_to_add, log_detail, base_date_str):
    log_detail(f"\n--- æ›´æ–°å†å²è®°å½•æ–‡ä»¶: {os.path.basename(file_path)} -> '{group_name}' ---")
    
    # ä½¿ç”¨ä¼ å…¥çš„åŸºå‡†æ—¥æœŸä½œä¸ºè®°å½•æ—¥æœŸ
    record_date_str = base_date_str

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {}

    if group_name not in data:
        data[group_name] = {}

    existing_symbols = data[group_name].get(record_date_str, [])
    combined_symbols = set(existing_symbols) | set(symbols_to_add)
    updated_symbols = sorted(list(combined_symbols))
    
    data[group_name][record_date_str] = updated_symbols
    num_added = len(updated_symbols) - len(existing_symbols)

    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        log_detail(f"æˆåŠŸæ›´æ–°å†å²è®°å½•ã€‚æ—¥æœŸ: {record_date_str}, åˆ†ç»„: '{group_name}'.")
        log_detail(f" - æœ¬æ¬¡æ–°å¢ {num_added} ä¸ªä¸é‡å¤çš„ symbolã€‚")
    except Exception as e:
        log_detail(f"é”™è¯¯: å†™å…¥å†å²è®°å½•æ–‡ä»¶å¤±è´¥: {e}")

# --- 3. æ ¸å¿ƒé€»è¾‘æ¨¡å— ---

def get_trading_dates_list(cursor, sector_name, symbol, end_date_str, limit=10):
    """
    è·å–åŒ…å« end_date_str åœ¨å†…çš„æœ€è¿‘ limit ä¸ªäº¤æ˜“æ—¥æ—¥æœŸåˆ—è¡¨ã€‚
    è¿”å›: ['2025-01-28', '2025-01-27', '2025-01-24', ...] (å€’åº)
    """
    query = f'SELECT date FROM "{sector_name}" WHERE name = ? AND date <= ? ORDER BY date DESC LIMIT ?'
    cursor.execute(query, (symbol, end_date_str, limit))
    rows = cursor.fetchall()
    return [r[0] for r in rows]

def check_volume_rank(cursor, sector_name, symbol, latest_date_str, latest_volume, lookback_months, rank_threshold, log_detail, is_tracing):
    """
    é€šç”¨æ£€æŸ¥ï¼šlatest_volume æ˜¯å¦æ˜¯è¿‡å» lookback_months ä¸ªæœˆå†…çš„å‰ rank_threshold å
    """
    # è®¡ç®— N ä¸ªæœˆå‰çš„æ—¥æœŸ
    try:
        dt = datetime.datetime.strptime(latest_date_str, "%Y-%m-%d")
        start_date = dt - datetime.timedelta(days=lookback_months * 30)
        start_date_str = start_date.strftime("%Y-%m-%d")
    except Exception:
        return False

    # æŸ¥è¯¢è¿‡å» N ä¸ªæœˆçš„æ‰€æœ‰æ—¥æœŸå’Œæˆäº¤é‡
    query = f'SELECT date, volume FROM "{sector_name}" WHERE name = ? AND date >= ? AND date <= ?'
    cursor.execute(query, (symbol, start_date_str, latest_date_str))
    rows = cursor.fetchall() # ç»“æœæ˜¯ [(date1, vol1), (date2, vol2), ...]
    
    # è¿‡æ»¤æ‰ None å€¼
    valid_data = [(r[0], r[1]) for r in rows if r[1] is not None]
    
    if not valid_data:
        return False
        
    # æ’åºï¼šæŒ‰ volume (x[1]) ä»å¤§åˆ°å°æ’
    sorted_data = sorted(valid_data, key=lambda x: x[1], reverse=True)
    
    # æˆªå–å‰ N å
    top_n_data = sorted_data[:rank_threshold]
    
    # æå–å‰ N åçš„æˆäº¤é‡æ•°å€¼
    top_n_volumes = [item[1] for item in top_n_data]
    
    # åˆ¤å®šé€»è¾‘ï¼šå½“å‰æˆäº¤é‡æ˜¯å¦åœ¨å‰ N åä¸­ï¼Œæˆ–è€…å¤§äºç­‰äºç¬¬ N åçš„å€¼
    is_top_n = False
    if latest_volume in top_n_volumes:
        is_top_n = True
    elif len(top_n_volumes) >= rank_threshold and latest_volume >= top_n_volumes[rank_threshold - 1]:
        is_top_n = True
        
    if is_tracing:
        log_detail(f"     [Rankæ£€æŸ¥] æ—¥æœŸ:{latest_date_str}, é‡:{latest_volume}, å›æº¯:{lookback_months}æœˆ")
        top_n_str = ", ".join([f"[{d}]: {v}" for d, v in top_n_data])
        log_detail(f"     å‰{rank_threshold}å: {top_n_str} -> ç»“æœ: {is_top_n}")
        
    return is_top_n

def check_is_earnings_day(cursor, symbol, target_date_str):
    """
    æ£€æŸ¥ target_date_str æ˜¯å¦ä¸ºè¯¥ symbol åœ¨ Earning è¡¨ä¸­çš„æœ€æ–°è´¢æŠ¥æ—¥ã€‚
    """
    try:
        # æŸ¥è¯¢è¯¥ symbol çš„æœ€æ–°ä¸€æ¡è´¢æŠ¥è®°å½•ï¼ˆæˆ–è€…ç›´æ¥æŸ¥æ˜¯å¦å­˜åœ¨è¯¥æ—¥æœŸçš„è®°å½•ï¼‰
        # è¿™é‡Œé€»è¾‘æ˜¯ï¼šå¦‚æœè¯¥æ—¥æ˜¯è´¢æŠ¥æ—¥ï¼ŒEarningè¡¨é‡Œåº”è¯¥æœ‰è¿™ä¸€å¤©çš„è®°å½•
        query = "SELECT date FROM Earning WHERE name = ? AND date = ?"
        cursor.execute(query, (symbol, target_date_str))
        row = cursor.fetchone()
        if row:
            return True
        return False
    except Exception as e:
        # å¦‚æœè¡¨ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å‡ºé”™ï¼Œé»˜è®¤ä¸è¿‡æ»¤
        return False

# --- ç­–ç•¥1: PE_Volume (T, T-1, T-2, T-3 æ”¾é‡ä¸‹è·Œ) ---
def process_condition_8(db_path, history_json_path, sector_map, target_date_override, symbol_to_trace, log_detail):
    """
    æ‰§è¡Œæ¡ä»¶8ç­–ç•¥ï¼šPE_Volume (ä¿®æ”¹ç‰ˆï¼šT-1, T-2, T-3 æ£€æŸ¥æ˜¯å¦æ”¾é‡ä¸”ä¸‹è·Œ)
    """
    log_detail("\n========== å¼€å§‹æ‰§è¡Œ æ¡ä»¶8 (PE_Volume - æ”¾é‡ä¸‹è·Œ) ç­–ç•¥ ==========")
    
    # è¯»å–é…ç½®
    rank_threshold = CONFIG.get("COND8_VOLUME_RANK_THRESHOLD", 3)
    lookback_months = CONFIG.get("COND8_VOLUME_LOOKBACK_MONTHS", 3)
    log_detail(f"é…ç½®å‚æ•°: æ’åé˜ˆå€¼ = Top {rank_threshold}, ä¸”å¿…é¡»æ”¶ç›˜ä»·ä¸‹è·Œ")

    # 1. ç¡®å®šåŸºå‡†æ—¥æœŸ (Today)
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œåˆ™è·å–æ˜¨å¤©çš„æ—¥æœŸ
    base_date = target_date_override if target_date_override else (datetime.date.today() - datetime.timedelta(days=1)).isoformat()
    log_detail(f"åŸºå‡†æ—¥æœŸ (Today): {base_date}")
    candidates_volume = set()
    
    # åŠ è½½å†å²è®°å½•
    try:
        with open(history_json_path, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
    except Exception as e:
        log_detail(f"é”™è¯¯: æ— æ³•è¯»å–å†å²è®°å½•æ–‡ä»¶: {e}")
        return []

    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(db_path, timeout=60.0)
    cursor = conn.cursor()
    target_groups = CONFIG["TARGET_GROUPS"]
    
    # è·å–å¤§ç›˜åŸºå‡†æ—¥æœŸ T-1, T-2, T-3
    sample_symbol = list(sector_map.keys())[0] if sector_map else "AAPL"
    sample_sector = sector_map.get(sample_symbol, "Technology")
    
    # è·å–æœ€è¿‘ 5 å¤©æ—¥æœŸï¼Œç®—å‡º T-1, T-2, T-3
    # global_dates[0] = Today, global_dates[1] = T-1 (ä¸Šä¸€ä¸ªæœ‰æ•ˆäº¤æ˜“æ—¥)
    global_dates = get_trading_dates_list(cursor, sample_sector, sample_symbol, base_date, limit=5)
    
    if len(global_dates) < 4:
        log_detail("é”™è¯¯: æ— æ³•è·å–è¶³å¤Ÿçš„äº¤æ˜“æ—¥å†æ•°æ®ã€‚")
        conn.close()
        return []
        
    # å®šä¹‰å…³é”®æ—¥æœŸ
    date_t0 = global_dates[0] # Today
    date_t1 = global_dates[1]
    date_t2 = global_dates[2]
    date_t3 = global_dates[3]
    
    log_detail(f"ç³»ç»Ÿè®¡ç®—å‡ºçš„å…³é”®æ—¥æœŸ: T={date_t0}, T-1={date_t1}, T-2={date_t2}, T-3={date_t3}")
    
    # å®šä¹‰ä»»åŠ¡åˆ—è¡¨
    # æ ¼å¼: (Target_Date_In_History, Date_Index_In_List, Task_Name)
    # Date_Index_In_List: 1ä»£è¡¨T-1, 2ä»£è¡¨T-2, 3ä»£è¡¨T-3
    tasks = [
        (date_t0, 0, "Tç­–ç•¥"),
        (date_t1, 1, "T-1ç­–ç•¥"),
        (date_t2, 2, "T-2ç­–ç•¥"),
        (date_t3, 3, "T-3ç­–ç•¥")
    ]
    
    for hist_date, date_idx, task_name in tasks:
        # 1. ä»å†å²æ–‡ä»¶ä¸­æå–è¯¥æ—¥æœŸçš„æ‰€æœ‰ symbol
        symbols_on_date = set()
        for group in target_groups:
            grp_data = history_data.get(group, {})
            if isinstance(grp_data, dict):
                syms = grp_data.get(hist_date, [])
                symbols_on_date.update(syms)
        
        symbols_on_date = sorted(list(symbols_on_date))
        log_detail(f" -> æ­£åœ¨æ‰«æ {task_name} (æ—¥æœŸ: {hist_date})ï¼ŒåŒ…å« {len(symbols_on_date)} ä¸ªå€™é€‰ã€‚")
        if symbol_to_trace:
            if symbol_to_trace in symbols_on_date:
                log_detail(f"    !!! ç›®æ ‡ {symbol_to_trace} åœ¨ {hist_date} çš„å†å²è®°å½•ä¸­ï¼Œå¼€å§‹æ£€æŸ¥...")
        
        for symbol in symbols_on_date:
            is_tracing = (symbol == symbol_to_trace)
            sector = sector_map.get(symbol)
            if not sector: continue
            
            # è·å–è¯¥è‚¡çš„å…·ä½“äº¤æ˜“æ—¥å†
            # è·å– 5 å¤©: Today(0), T-1(1), T-2(2), T-3(3)
            dates = get_trading_dates_list(cursor, sector, symbol, base_date, limit=5)
            
            if len(dates) < 4: continue
            if dates[date_idx] != hist_date: continue
            
            # ========== ä¿®æ”¹ç‚¹ï¼šè·å–ä»Šæ—¥(dates[0]) å’Œ æ˜¨æ—¥(dates[1]) çš„ä»·æ ¼å’Œæˆäº¤é‡ ==========
            # æŸ¥è¯¢æœ€è¿‘ä¸¤å¤©çš„æ•°æ® (å€’åº: Row 0=Today, Row 1=Yesterday)
            query = f'SELECT price, volume FROM "{sector}" WHERE name = ? AND date <= ? ORDER BY date DESC LIMIT 2'
            cursor.execute(query, (symbol, dates[0]))
            rows = cursor.fetchall()
            
            if len(rows) < 2:
                if is_tracing: log_detail(f"    x [å¤±è´¥] ç¼ºå°‘è¶³å¤Ÿçš„ä»·æ ¼æ•°æ®è¿›è¡Œæ¶¨è·Œå¹…å¯¹æ¯”ã€‚")
                continue
            
            price_curr, vol_curr = rows[0]
            price_prev, vol_prev = rows[1]
            
            if price_curr is None or price_prev is None or vol_curr is None: continue

            # ========== è§„åˆ™ä¿®æ”¹ï¼šå¿…é¡»ä¸‹è·Œ (ä»Šæ—¥ä»· < æ˜¨æ—¥ä»·) ==========
            if price_curr >= price_prev:
                if is_tracing: log_detail(f"    x [å¤±è´¥] ä»·æ ¼æœªä¸‹è·Œ ({price_curr} >= {price_prev})ã€‚")
                continue

            # æ ¸å¿ƒåˆ¤æ–­é€»è¾‘ï¼šæ£€æŸ¥æ”¾é‡
            vol_cond = check_volume_rank(
                cursor, sector, symbol, dates[0], vol_curr, 
                CONFIG["COND8_VOLUME_LOOKBACK_MONTHS"], 
                rank_threshold, 
                log_detail, is_tracing
            )
            
            if vol_cond:
                # ================== è´¢æŠ¥æ—¥è¿‡æ»¤é€»è¾‘ ==================
                # æ£€æŸ¥ä»Šæ—¥(dates[0])æ˜¯å¦ä¸ºè´¢æŠ¥æ—¥
                if check_is_earnings_day(cursor, symbol, dates[0]):
                    if is_tracing: log_detail(f"    ğŸ›‘ [è¿‡æ»¤] ä»Šæ—¥({dates[0]}) ä¸ºè´¢æŠ¥æ—¥ï¼Œå‰”é™¤ã€‚")
                    continue

                candidates_volume.add(symbol)
                if is_tracing: log_detail(f"    âœ… [é€šè¿‡] {task_name} æ”¾é‡ä¸‹è·Œæ¡ä»¶æ»¡è¶³ï¼(Price: {price_prev}->{price_curr})")

    conn.close()
    
    result_list = sorted(list(candidates_volume))
    log_detail(f"æ¡ä»¶8 (PE_Volume) ç­›é€‰å®Œæˆï¼Œå…±å‘½ä¸­ {len(result_list)} ä¸ª: {result_list}")
    return result_list

# --- ç­–ç•¥2: PE_Volume_up (T, T-1, T-2 æ´»è·ƒä¸”ä»Šæ—¥ä¸Šæ¶¨) ---
def process_pe_volume_up(db_path, history_json_path, sector_map, target_date_override, symbol_to_trace, log_detail):
    log_detail("\n========== å¼€å§‹æ‰§è¡Œ ç­–ç•¥2 (PE_Volume_up) ==========")
    
    # é…ç½®å‚æ•°
    lookback_days = CONFIG.get("COND_UP_HISTORY_LOOKBACK_DAYS", 3) 
    # ä¿®æ”¹ç‚¹ï¼šæ”¾é‡æ£€æŸ¥å›æº¯æœˆä»½æ”¹ä¸º3ä¸ªæœˆ
    vol_rank_months = CONFIG.get("COND_UP_VOL_RANK_MONTHS", 3)
    vol_rank_threshold = CONFIG.get("COND_UP_VOL_RANK_THRESHOLD", 3)
    
    log_detail(f"é…ç½®: å†å²æ± æ‰«æèŒƒå›´=è¿‘3å¤©(T, T-1, T-2), æ”¾é‡æ ‡å‡†=è¿‘{vol_rank_months}ä¸ªæœˆå‰{vol_rank_threshold}å")

    # ã€å›æµ‹é€»è¾‘ã€‘è¿™é‡Œå¤„ç†å›æµ‹æ—¥æœŸ
    base_date = target_date_override if target_date_override else (datetime.date.today() - datetime.timedelta(days=1)).isoformat()
    
    # 1. è¿æ¥æ•°æ®åº“è·å–å…¨å±€æ—¥æœŸ
    conn = sqlite3.connect(db_path, timeout=60.0)
    cursor = conn.cursor()
    
    sample_symbol = list(sector_map.keys())[0] if sector_map else "AAPL"
    sample_sector = sector_map.get(sample_symbol, "Technology")
    # è·å–æœ€è¿‘3ä¸ªäº¤æ˜“æ—¥ (T, T-1, T-2)
    global_dates = get_trading_dates_list(cursor, sample_sector, sample_symbol, base_date, limit=lookback_days)
    
    if len(global_dates) < 2: # è‡³å°‘éœ€è¦ T å’Œ T-1
        log_detail("é”™è¯¯: äº¤æ˜“æ—¥æ•°æ®ä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œç­–ç•¥2ã€‚")
        conn.close()
        return []
    
    log_detail(f"æ‰«æå†å²æ—¥æœŸèŒƒå›´ (T, T-1, T-2): {global_dates}")

    # 2. ä»Historyä¸­æ”¶é›†å€™é€‰è‚¡ (ä»…é™ T, T-1, T-2)
    try:
        with open(history_json_path, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
    except Exception:
        conn.close()
        return []

    target_groups = CONFIG["TARGET_GROUPS"]
    candidate_symbols = set()
    
    for hist_date in global_dates:
        for group in target_groups:
            grp_data = history_data.get(group, {})
            if isinstance(grp_data, dict):
                syms = grp_data.get(hist_date, [])
                candidate_symbols.update(syms)
    
    candidate_symbols = sorted(list(candidate_symbols))
    log_detail(f"åœ¨ T, T-1, T-2 çš„å†å²è®°å½•ä¸­å…±æ‰«æåˆ° {len(candidate_symbols)} ä¸ªå€™é€‰ Symbolã€‚")

    results = []
    
    # 3. é€ä¸ªæ£€æŸ¥é€»è¾‘
    for symbol in candidate_symbols:
        is_tracing = (symbol == symbol_to_trace)
        sector = sector_map.get(symbol)
        if not sector: continue
        
        if is_tracing: log_detail(f"--- æ­£åœ¨æ£€æŸ¥ {symbol} (ç­–ç•¥2) ---")

        # ã€ä¿®æ”¹ç‚¹ 1ã€‘å°† LIMIT ä» 3 æ”¹ä¸º 8ï¼Œä»¥ä¾¿è·å–ä»Šæ—¥ + è¿‡å» 7 å¤©çš„æ•°æ®
        query = f'SELECT date, price, volume FROM "{sector}" WHERE name = ? AND date <= ? ORDER BY date DESC LIMIT 8'
        cursor.execute(query, (symbol, base_date))
        rows = cursor.fetchall()
        
        # è‡³å°‘éœ€è¦ T å’Œ T-1 è¿›è¡Œæ¶¨è·Œåˆ¤æ–­
        if len(rows) < 2:
            if is_tracing: log_detail(f"    x æ•°æ®ä¸è¶³2å¤©ï¼Œè·³è¿‡ã€‚")
            continue
            
        # rows[0]=T, rows[1]=T-1, rows[2]=T-2 (å¯èƒ½ä¸å­˜åœ¨)
        # æå–æ•°æ®
        date_curr, price_curr, vol_curr = rows[0]
        date_prev, price_prev, vol_prev = rows[1]
        
        if price_curr is None or price_prev is None or vol_curr is None or vol_prev is None:
            continue

        # è§„åˆ™1 (ç¡¬æ€§): å¿…é¡»ä¸Šæ¶¨ (æœ€æ–°ä»· > æ¬¡æ–°ä»·)
        if price_curr <= price_prev:
            if is_tracing: log_detail(f"    x ä»·æ ¼æœªä¸Šæ¶¨ ({price_curr} <= {price_prev})ï¼Œè·³è¿‡ã€‚")
            continue

        # ã€ä¿®æ”¹ç‚¹ 2ã€‘æ–°å¢è¿‡æ»¤ï¼šæ¯”å‰ 7 å¤©æœ€ä½ç‚¹é«˜å‡º 3% åˆ™è¿‡æ»¤
        # æå– rows[1:] ä¸­çš„æ‰€æœ‰ä»·æ ¼ï¼ˆå³æ’é™¤ä»Šæ—¥åçš„å‰ 7 å¤©ï¼‰
        past_prices = [r[1] for r in rows[1:] if r[1] is not None]
        if past_prices:
            min_past_price = min(past_prices)
            threshold_price = min_past_price * 1.03
            if price_curr > threshold_price:
                if is_tracing: 
                    log_detail(f"    ğŸ›‘ [è¿‡æ»¤] æ¶¨å¹…è¿‡å¤§: å½“å‰ä»· {price_curr} è¶…è¿‡å‰{len(past_prices)}æ—¥æœ€ä½ç‚¹ {min_past_price} çš„ 3% (é˜ˆå€¼: {threshold_price:.2f})")
                continue
            else:
                if is_tracing:
                    log_detail(f"    i [é€šè¿‡] ä»·æ ¼ä½ç½®åˆç†: å½“å‰ä»· {price_curr} æœªè¶…è¿‡å‰{len(past_prices)}æ—¥æœ€ä½ç‚¹ {min_past_price} çš„ 3%")

        # è§„åˆ™2: è´¢æŠ¥æ—¥è¿‡æ»¤ (T-1æ—¥)
        if check_is_earnings_day(cursor, symbol, date_prev):
            if is_tracing: log_detail(f"    ğŸ›‘ æ˜¨æ—¥({date_prev})æ˜¯è´¢æŠ¥æ—¥ï¼Œè·³è¿‡ã€‚")
            continue

        is_match = False
        reason = ""

        # è§„åˆ™3: æˆäº¤é‡åˆ†æ”¯é€»è¾‘
        if vol_curr > vol_prev:
            # === åˆ†æ”¯ A: æ”¾é‡ä¸Šæ¶¨ ===
            # ä¿®æ”¹ç‚¹: æ£€æŸ¥ä»Šæ—¥(T)æ˜¯å¦ä¸º 3ä¸ªæœˆå†…å‰3å
            is_top_vol = check_volume_rank(
                cursor, sector, symbol, date_curr, vol_curr, 
                vol_rank_months, vol_rank_threshold, log_detail, is_tracing
            )
            if is_top_vol:
                is_match = True
                reason = "æ”¾é‡ä¸Šæ¶¨ (3ä¸ªæœˆTop3)"
            else:
                if is_tracing: log_detail(f"    x æ”¾é‡ä½†æœªæ»¡è¶³3ä¸ªæœˆTop{vol_rank_threshold}ã€‚")
        else:
            # === åˆ†æ”¯ B: ç¼©é‡ä¸Šæ¶¨ ===
            # ä¿®æ”¹ç‚¹: æ£€æŸ¥ T, T-1, T-2 ä¸­æ˜¯å¦æœ‰ä»»æ„ä¸€å¤©æ˜¯â€œ3ä¸ªæœˆå†…å‰3åâ€
            # å·²ç»æ»¡è¶³: é‡ç¼© (vol_curr < vol_prev) ä¸” ä»·æ¶¨ (price_curr > price_prev)
            
            # æ£€æŸ¥åˆ—è¡¨ä¸­çš„æ¯ä¸€å¤© (T, T-1, T-2)
            has_high_volume_history = False
            # æ£€æŸ¥ T, T-1, T-2
            for i in range(min(3, len(rows))):
                d_date, _, d_vol = rows[i]
                if d_vol is None: continue
                
                # æ£€æŸ¥è¿™ä¸€å¤©æ˜¯å¦æ˜¯å½“æ—¶çš„3ä¸ªæœˆå†…å‰3å
                # æ³¨æ„ï¼šcheck_volume_rank ä¼šè‡ªåŠ¨å›æº¯è¯¥æ—¥æœŸä¹‹å‰çš„3ä¸ªæœˆ
                is_high = check_volume_rank(
                    cursor, sector, symbol, d_date, d_vol,
                    vol_rank_months, vol_rank_threshold, log_detail, False # è¿™é‡Œå¦‚æœä¸è¿½è¸ªç»†èŠ‚å¯ä»¥è®¾ä¸ºFalseï¼Œé¿å…æ—¥å¿—çˆ†ç‚¸
                )
                if is_high:
                    has_high_volume_history = True
                    if is_tracing: log_detail(f"    -> å‘ç°é«˜é‡æ—¥: {d_date} (Vol:{d_vol})")
                    break # åªè¦æœ‰ä¸€å¤©æ»¡è¶³å³å¯
            
            if has_high_volume_history:
                is_match = True
                reason = "ç¼©é‡ä¸Šæ¶¨ (è¿‘3æ—¥å­˜åœ¨é«˜é‡)"
            else:
                if is_tracing: log_detail(f"    x ç¼©é‡ä¸Šæ¶¨ï¼Œä½†è¿‘3æ—¥(T,T-1,T-2)å‡æ— é«˜é‡è®°å½•ã€‚")

        if is_match:
            results.append(symbol)
            if is_tracing: log_detail(f"    âœ… [é€‰ä¸­] {reason}")

    conn.close()
    log_detail(f"ç­–ç•¥2 (PE_Volume_up) ç­›é€‰å®Œæˆï¼Œå…±å‘½ä¸­ {len(results)} ä¸ªã€‚")
    return sorted(results)

# --- 4. ä¸»æ‰§è¡Œæµç¨‹ ---

def run_pe_volume_logic(log_detail):
    log_detail("PE_Volume åŒç­–ç•¥ç¨‹åºå¼€å§‹è¿è¡Œ...")
    if SYMBOL_TO_TRACE: log_detail(f"å½“å‰è¿½è¸ªçš„ SYMBOL: {SYMBOL_TO_TRACE}")
    
    base_date_str = TARGET_DATE if TARGET_DATE else (datetime.date.today() - datetime.timedelta(days=1)).isoformat()

    if TARGET_DATE:
        log_detail(f"\nâš ï¸âš ï¸âš ï¸ æ³¨æ„ï¼šå½“å‰å¤„äºã€å›æµ‹æ¨¡å¼ã€‘ï¼Œç›®æ ‡æ—¥æœŸï¼š{TARGET_DATE} âš ï¸âš ï¸âš ï¸")
        log_detail("æœ¬æ¬¡è¿è¡Œå°†ã€ä¸ä¼šã€‘æ›´æ–° Panel å’Œ History JSON æ–‡ä»¶ã€‚")
    
    # 1. åŠ è½½é…ç½®å’Œæ˜ å°„
    tag_blacklist, hot_tags = load_tag_settings(TAGS_SETTING_JSON_FILE)
    symbol_to_sector_map = load_all_symbols(SECTORS_JSON_FILE, CONFIG["TARGET_SECTORS"])
    symbol_to_tags_map = load_symbol_tags(DESCRIPTION_JSON_FILE)

    if not symbol_to_sector_map:
        log_detail("é”™è¯¯: æ— æ³•åŠ è½½æ¿å—æ˜ å°„ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    # ================= ç­–ç•¥ 1 æ‰§è¡Œ (å·²æ¢å¤) =================
    # æ‰§è¡Œç­–ç•¥1ï¼šæ”¾é‡ä¸‹è·Œ
    raw_pe_volume = process_condition_8(
        DB_FILE, 
        EARNING_HISTORY_JSON_FILE, 
        symbol_to_sector_map, 
        TARGET_DATE, 
        SYMBOL_TO_TRACE, 
        log_detail
    )
    final_pe_volume = sorted(list(set(raw_pe_volume)))

    # ================= ç­–ç•¥ 2 æ‰§è¡Œ =================
    # ä¼ å…¥ TARGET_DATEï¼Œå†…éƒ¨ä¼šå¤„ç†
    raw_pe_volume_up = process_pe_volume_up(
        DB_FILE,
        EARNING_HISTORY_JSON_FILE,
        symbol_to_sector_map,
        TARGET_DATE, 
        SYMBOL_TO_TRACE,
        log_detail
    )
    final_pe_volume_up = sorted(list(set(raw_pe_volume_up)))

    # ================= Tag é»‘åå•è¿‡æ»¤é€»è¾‘ =================
    def filter_blacklisted_tags(symbols):
        allowed = []
        for sym in symbols:
            # è·å–è¯¥è‚¡çš„ tags
            s_tags = set(symbol_to_tags_map.get(sym, []))
            # æ£€æŸ¥æ˜¯å¦æœ‰äº¤é›† (å³æ˜¯å¦å‘½ä¸­é»‘åå•)
            intersect = s_tags.intersection(tag_blacklist)
            if not intersect:
                allowed.append(sym)
            else:
                # å¦‚æœæ˜¯è¿½è¸ªç›®æ ‡ï¼Œæ‰“å°æ—¥å¿—
                if sym == SYMBOL_TO_TRACE:
                    log_detail(f"ğŸ›‘ [Tagè¿‡æ»¤] {sym} å‘½ä¸­é»‘åå•æ ‡ç­¾: {intersect} -> å‰”é™¤ã€‚")
        return sorted(allowed)

    # å¯¹ç­–ç•¥ç»“æœè¿›è¡Œè¿‡æ»¤ (ç”¨äºå†™å…¥ Panel)
    # ç­–ç•¥ 1 (è™½ç„¶ç°åœ¨ä¸ºç©ºï¼Œä½†é€»è¾‘åŠ ä¸Š)
    filtered_pe_volume = filter_blacklisted_tags(final_pe_volume)
    
    # ç­–ç•¥ 2
    filtered_pe_volume_up = filter_blacklisted_tags(final_pe_volume_up)
    
    if SYMBOL_TO_TRACE:
        if SYMBOL_TO_TRACE in final_pe_volume and SYMBOL_TO_TRACE not in filtered_pe_volume:
             log_detail(f"è¿½è¸ªæç¤º: {SYMBOL_TO_TRACE} (ç­–ç•¥1) é€šè¿‡ï¼Œä½†å› é»‘åå•æ ‡ç­¾è¢«è¿‡æ»¤ã€‚")
        if SYMBOL_TO_TRACE in final_pe_volume_up and SYMBOL_TO_TRACE not in filtered_pe_volume_up:
             log_detail(f"è¿½è¸ªæç¤º: {SYMBOL_TO_TRACE} (ç­–ç•¥2) é€šè¿‡ï¼Œä½†å› é»‘åå•æ ‡ç­¾è¢«è¿‡æ»¤ã€‚")

    # ================= [æ–°å¢é€»è¾‘] æ£€æŸ¥ PE_Deep / PE_Deeper äº¤å‰ =================
    # åœ¨ç”Ÿæˆ Note ä¹‹å‰ï¼Œå…ˆè¯»å–ç°æœ‰çš„ Panel æ–‡ä»¶ï¼Œæ‰¾å‡ºå“ªäº› symbol åœ¨ Deep/Deeper ç»„é‡Œ
    all_existing_notes = {}
    current_deep_symbols = set()
    try:
        with open(PANEL_JSON_FILE, 'r', encoding='utf-8') as f:
            p_data = json.load(f)
            # æ”¶é›†æ‰€æœ‰ç»„çš„å¤‡æ³¨ï¼Œé˜²æ­¢è¦†ç›–
            for group_name, group_content in p_data.items():
                if isinstance(group_content, dict):
                    for s, n in group_content.items():
                        # å¦‚æœå¤‡æ³¨é‡Œæœ‰ä¸œè¥¿ï¼Œå°±å­˜ä¸‹æ¥
                        if len(n) > len(all_existing_notes.get(s, "")):
                            all_existing_notes[s] = n
            
            # ä¸“é—¨æå– Deep/Deeper ç”¨äºâ€œå¬â€å­—é€»è¾‘
            if "PE_Deep" in p_data: current_deep_symbols.update(p_data["PE_Deep"].keys())
            if "PE_Deeper" in p_data: current_deep_symbols.update(p_data["PE_Deeper"].keys())
            
            # === ä¿®æ”¹ç‚¹ï¼šæ–°å¢ PE_valid å’Œ PE_invalid ç”¨äºâ€œå¬â€å­—é€»è¾‘ ===
            if "PE_valid" in p_data: current_deep_symbols.update(p_data["PE_valid"].keys())
            if "PE_invalid" in p_data: current_deep_symbols.update(p_data["PE_invalid"].keys())
            if "OverSell_W" in p_data: current_deep_symbols.update(p_data["OverSell_W"].keys())
            
    except Exception as e:
        log_detail(f"æç¤º: è¯»å–ç°æœ‰å¤‡æ³¨æ—¶å‡ºé”™(å¯èƒ½æ˜¯æ–‡ä»¶ä¸å­˜åœ¨): {e}")

    # 4. æ„å»ºå¤‡æ³¨ (Note) - ä½¿ç”¨è¿‡æ»¤åçš„åˆ—è¡¨
    # ä¿®æ”¹ï¼šå¢åŠ  highlight_set å‚æ•°ï¼Œç”¨äºç»™ç‰¹å®šé›†åˆä¸­çš„ symbol åŠ  "å¬" åç¼€
    def build_symbol_note_map(symbols, existing_notes=None, highlight_set=None):
        """
        symbols: æœ¬æ¬¡ç­›é€‰å‡ºçš„ symbol åˆ—è¡¨
        existing_notes: å­—å…¸ï¼Œå­˜å‚¨äº†ä» panel.json è¯»å–çš„ {symbol: "åŸæœ‰å¤‡æ³¨"}
        highlight_set: Deep/Deeper/Valid/Invalid çš„ symbol é›†åˆ
        """
        note_map = {}
        for sym in symbols:
            # 1. è·å–åŸæœ‰å¤‡æ³¨ï¼ˆå¦‚ "OKLO15çƒ­"ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬é€šå¸¸åªéœ€è¦åç¼€éƒ¨åˆ†ï¼Œæ‰€ä»¥æŠŠ symbol åˆ æ‰
            orig_note = ""
            if existing_notes and sym in existing_notes:
                orig_note = existing_notes[sym].replace(sym, "") # æå–å‡º "15çƒ­"
            
            # 3. æ„é€ æ–°å¤‡æ³¨
            new_suffix = orig_note
            
            # æ£€æŸ¥â€œå¬â€ï¼šå¦‚æœå±äº Deep ç»„ä¸”å½“å‰å¤‡æ³¨é‡Œæ²¡â€œå¬â€
            if highlight_set and sym in highlight_set:
                if "å¬" not in new_suffix:
                    new_suffix += "å¬"
            
            # æœ€ç»ˆç»„åˆï¼šSymbol + ç´¯åŠ åçš„åç¼€
            note_map[sym] = f"{sym}{new_suffix}"
        return note_map
        
    # ä¸º PE_Volume ç»„ç”Ÿæˆå¤‡æ³¨
    pe_volume_notes = build_symbol_note_map(
        filtered_pe_volume, 
        existing_notes=all_existing_notes, 
        highlight_set=current_deep_symbols
    )
    
    # PE_Volume_up æš‚æ—¶ä¸éœ€è¦æ­¤é€»è¾‘
    pe_volume_up_notes = build_symbol_note_map(filtered_pe_volume_up)

    # 5. å›æµ‹å®‰å…¨æ‹¦æˆª
    # ã€å›æµ‹é€»è¾‘ã€‘å¦‚æœè®¾ç½®äº† TARGET_DATEï¼Œåœ¨è¿™é‡Œç›´æ¥ returnï¼Œä¸æ‰§è¡Œä¸‹é¢çš„å†™å…¥æ“ä½œ
    if TARGET_DATE:
        log_detail("\n" + "="*60)
        log_detail(f"ğŸ›‘ [å®‰å…¨æ‹¦æˆª] å›æµ‹æ¨¡å¼ (Date: {TARGET_DATE}) å·²å¯ç”¨ã€‚")
        log_detail(f"ğŸ“Š [ç­–ç•¥1] PE_Volume (æ”¾é‡ä¸‹è·Œ) å‘½ä¸­: {len(filtered_pe_volume)} ä¸ª (Raw: {len(final_pe_volume)})") 
        log_detail(f"ğŸ“Š [ç­–ç•¥2] PE_Volume_up (æ´»è·ƒä¸Šæ¶¨) å‘½ä¸­: {len(filtered_pe_volume_up)} ä¸ª (Raw: {len(final_pe_volume_up)})")
        log_detail("="*60 + "\n")
        return

    # 6. å†™å…¥ Panel (ä½¿ç”¨è¿‡æ»¤åçš„ clean data)
    log_detail(f"\næ­£åœ¨å†™å…¥ Panel æ–‡ä»¶...")
    
    # ä½¿ç”¨æ–°çš„å‡½æ•°ï¼šåŒæ—¶å†™å…¥ Volume åˆ†ç»„å¹¶æ¸…ç† Backup å†²çª
    update_panel_with_conflict_check(
        PANEL_JSON_FILE,
        filtered_pe_volume, pe_volume_notes,
        filtered_pe_volume_up, pe_volume_up_notes,
        log_detail
    )

    # 7. å†™å…¥ History (é€šå¸¸ä¿ç•™ Raw Data)
    log_detail(f"æ­£åœ¨æ›´æ–° History æ–‡ä»¶...")
    # ç­–ç•¥1 å†™å…¥ (æ¢å¤)
    update_earning_history_json(EARNING_HISTORY_JSON_FILE, "PE_Volume", final_pe_volume, log_detail, base_date_str)
    
    # ç­–ç•¥2 å†™å…¥ (ä½¿ç”¨åŸå§‹ Raw Dataï¼Œä¿æŒç®—æ³•æ± å®Œæ•´æ€§)
    update_earning_history_json(EARNING_HISTORY_JSON_FILE, "PE_Volume_up", final_pe_volume_up, log_detail, base_date_str)

    # ================= [æ–°å¢] å†™å…¥ Tag é»‘åå•æ ‡è®°åˆ†ç»„ =================
    # æ±‡æ€»ä¸¤ä¸ªç­–ç•¥çš„æ‰€æœ‰åŸå§‹ Symbol (æ³¨æ„ï¼šè¿™é‡Œç”¨çš„æ˜¯ final_pe_volume ç­‰åŸå§‹åˆ—è¡¨ï¼Œæœªç»è¿‡ tag å‰”é™¤çš„)
    all_volume_symbols = set(final_pe_volume) | set(final_pe_volume_up)
    
    blocked_symbols_to_log = []
    # tag_blacklist åœ¨å‡½æ•°å¼€å¤´å·²ç»åŠ è½½
    
    for sym in all_volume_symbols:
        s_tags = set(symbol_to_tags_map.get(sym, []))
        if s_tags.intersection(tag_blacklist):
            blocked_symbols_to_log.append(sym)
            
    if blocked_symbols_to_log:
        blocked_symbols_to_log = sorted(list(set(blocked_symbols_to_log)))
        update_earning_history_json(EARNING_HISTORY_JSON_FILE, "_Tag_Blacklist", blocked_symbols_to_log, log_detail, base_date_str)
        log_detail(f"å·²å°† {len(blocked_symbols_to_log)} ä¸ªå‘½ä¸­é»‘åå•Tagçš„symbolé¢å¤–è®°å…¥ '_Tag_Blacklist' åˆ†ç»„ã€‚")
    # ================================================================

    log_detail("ç¨‹åºè¿è¡Œç»“æŸã€‚")

def main():
    if SYMBOL_TO_TRACE:
        print(f"è¿½è¸ªæ¨¡å¼å·²å¯ç”¨ï¼Œç›®æ ‡: {SYMBOL_TO_TRACE}ã€‚æ—¥å¿—å°†å†™å…¥: {LOG_FILE_PATH}")
        try:
            with open(LOG_FILE_PATH, 'w', encoding='utf-8') as log_file:
                def log_detail_file(message):
                    log_file.write(message + '\n')
                    print(message)
                run_pe_volume_logic(log_detail_file)
        except IOError as e:
            print(f"é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æˆ–å†™å…¥æ—¥å¿—æ–‡ä»¶ {LOG_FILE_PATH}: {e}")
    else:
        print("è¿½è¸ªæ¨¡å¼æœªå¯ç”¨ã€‚æ—¥å¿—ä»…è¾“å‡ºåˆ°æ§åˆ¶å°ã€‚")
        def log_detail_console(message):
            print(message)
        run_pe_volume_logic(log_detail_console)

if __name__ == '__main__':
    main()