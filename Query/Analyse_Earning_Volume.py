import json
import sqlite3
import os
import datetime

USER_HOME = os.path.expanduser("~")
BASE_CODING_DIR = os.path.join(USER_HOME, "Coding")

# --- 1. é…ç½®æ–‡ä»¶å’Œè·¯å¾„ ---
BASE_PATH = USER_HOME

# ================= é…ç½®åŒºåŸŸ =================
SYMBOL_TO_TRACE = "" 
TARGET_DATE = ""

# SYMBOL_TO_TRACE = "LRN"
# TARGET_DATE = "2025-11-11"

# 3. æ—¥å¿—è·¯å¾„
LOG_FILE_PATH = os.path.join(BASE_PATH, "Downloads", "PE_Volume_trace_log.txt")

PATHS = {
    "config_dir": os.path.join(BASE_CODING_DIR, 'Financial_System', 'Modules'),
    "db_dir": os.path.join(BASE_CODING_DIR, 'Database'),
    "sectors_json": lambda config_dir: os.path.join(config_dir, 'Sectors_All.json'),
    "panel_json": lambda config_dir: os.path.join(config_dir, 'Sectors_panel.json'),
    "description_json": lambda config_dir: os.path.join(config_dir, 'description.json'),
    "tags_setting_json": lambda config_dir: os.path.join(config_dir, 'tags_filter.json'),
    "earnings_history_json": lambda config_dir: os.path.join(config_dir, 'Earning_History.json'),
    "db_file": lambda db_dir: os.path.join(db_dir, 'Finance.db'),
}

# åŠ¨æ€ç”Ÿæˆå®Œæ•´è·¯å¾„
CONFIG_DIR = PATHS["config_dir"]
DB_DIR = PATHS["db_dir"]
DB_FILE = PATHS["db_file"](DB_DIR)
SECTORS_JSON_FILE = PATHS["sectors_json"](CONFIG_DIR)
PANEL_JSON_FILE = PATHS["panel_json"](CONFIG_DIR)
DESCRIPTION_JSON_FILE = PATHS["description_json"](CONFIG_DIR)
TAGS_SETTING_JSON_FILE = PATHS["tags_setting_json"](CONFIG_DIR)
EARNING_HISTORY_JSON_FILE = PATHS["earnings_history_json"](CONFIG_DIR)

CONFIG = {
    "TARGET_SECTORS": {
        "Basic_Materials", "Communication_Services", "Consumer_Cyclical",
        "Consumer_Defensive", "Energy", "Financial_Services", "Healthcare",
        "Industrials", "Real_Estate", "Technology", "Utilities"
    },
    # ========== æ¡ä»¶8 (PE_Volume) å‚æ•° ==========
    "COND8_VOLUME_LOOKBACK_MONTHS": 3,   # è¿‡å»3ä¸ªæœˆ
    "COND8_VOLUME_RANK_THRESHOLD": 3,    # ã€æ–°å¢é…ç½®ã€‘æˆäº¤é‡æ’åå‰ N å (é»˜è®¤3)
    "COND8_TARGET_GROUPS": [             # éœ€è¦å»å†å²æ–‡ä»¶ä¸­æ‰«æçš„åˆ†ç»„
        "OverSell_W", "PE_Deeper", "PE_Deep", 
        "PE_W", "PE_valid", "PE_invalid", "season", "no_season"
    ],
}

# --- 2. è¾…åŠ©ä¸æ–‡ä»¶æ“ä½œæ¨¡å— ---

def load_tag_settings(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            settings = json.load(f)
        tag_blacklist = set(settings.get('BLACKLIST_TAGS', []))
        hot_tags = set(settings.get('HOT_TAGS', []))
        return tag_blacklist, hot_tags
    except Exception:
        return set(), set()

def load_all_symbols(json_path, target_sectors):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            all_sectors_data = json.load(f)
        symbol_to_sector_map = {}
        for sector, symbols in all_sectors_data.items():
            if sector in target_sectors:
                for symbol in symbols:
                    symbol_to_sector_map[symbol] = sector
        return symbol_to_sector_map
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½symbolså¤±è´¥: {e}")
        return None

def load_symbol_tags(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        symbol_tag_map = {}
        stock_list = data.get('stocks', [])
        for item in stock_list:
            symbol = item.get('symbol')
            tags = item.get('tag', [])
            if symbol:
                symbol_tag_map[symbol] = tags
        return symbol_tag_map
    except Exception:
        return {}

def update_json_panel(symbols_list, json_path, group_name, symbol_to_note=None):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {}

    if symbol_to_note is None:
        data[group_name] = {symbol: "" for symbol in sorted(symbols_list)}
    else:
        data[group_name] = {symbol: symbol_to_note.get(symbol, "") for symbol in sorted(symbols_list)}

    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"é”™è¯¯: å†™å…¥JSONæ–‡ä»¶å¤±è´¥: {e}")

def update_earning_history_json(file_path, group_name, symbols_to_add, log_detail):
    log_detail(f"\n--- æ›´æ–°å†å²è®°å½•æ–‡ä»¶: {os.path.basename(file_path)} -> '{group_name}' ---")
    
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ TARGET_DATE ä½œä¸ºè®°å½•æ—¥æœŸï¼Œå¦‚æœä¸ºç©ºåˆ™ç”¨æ˜¨å¤©ï¼ˆåŸé€»è¾‘ä¹ æƒ¯ï¼‰
    # ä½†å¯¹äº PE_Volume è¿™ç§å½“æ—¥ç­–ç•¥ï¼Œé€šå¸¸è®°å½•åœ¨å½“æ—¥ã€‚
    # ä¸ºäº†ä¿æŒä¸åŸä»£ç ä¸€è‡´æ€§ï¼ˆåŸä»£ç æ˜¯ç”¨ yesterday è®°å½•ï¼‰ï¼Œè¿™é‡Œä¿æŒåŸæ ·ï¼Œ
    yesterday = datetime.date.today() - datetime.timedelta(days=1)
    record_date_str = yesterday.isoformat()
    
    # å¦‚æœæ˜¯å›æµ‹æ¨¡å¼ï¼Œç†è®ºä¸Šä¸åº”è¯¥å†™å…¥ï¼Œä½†å¦‚æœå¼ºåˆ¶å†™å…¥ï¼Œåº”ä½¿ç”¨å›æµ‹æ—¥æœŸ
    if TARGET_DATE:
        record_date_str = TARGET_DATE

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {}

    if group_name not in data:
        data[group_name] = {}

    existing_symbols = data[group_name].get(record_date_str, [])
    combined_symbols = set(existing_symbols) | set(symbols_to_add)
    updated_symbols = sorted(list(combined_symbols))
    
    data[group_name][record_date_str] = updated_symbols
    num_added = len(updated_symbols) - len(existing_symbols)

    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        log_detail(f"æˆåŠŸæ›´æ–°å†å²è®°å½•ã€‚æ—¥æœŸ: {record_date_str}, åˆ†ç»„: '{group_name}'.")
        log_detail(f" - æœ¬æ¬¡æ–°å¢ {num_added} ä¸ªä¸é‡å¤çš„ symbolã€‚")
    except Exception as e:
        log_detail(f"é”™è¯¯: å†™å…¥å†å²è®°å½•æ–‡ä»¶å¤±è´¥: {e}")

# --- 3. æ ¸å¿ƒé€»è¾‘æ¨¡å— (Condition 8) ---

def get_trading_dates_list(cursor, sector_name, symbol, end_date_str, limit=10):
    """
    è·å–åŒ…å« end_date_str åœ¨å†…çš„æœ€è¿‘ limit ä¸ªäº¤æ˜“æ—¥æ—¥æœŸåˆ—è¡¨ã€‚
    è¿”å›: ['2025-01-28', '2025-01-27', '2025-01-24', ...] (å€’åº)
    """
    query = f'SELECT date FROM "{sector_name}" WHERE name = ? AND date <= ? ORDER BY date DESC LIMIT ?'
    cursor.execute(query, (symbol, end_date_str, limit))
    rows = cursor.fetchall()
    return [r[0] for r in rows]

def check_volume_rank(cursor, sector_name, symbol, latest_date_str, latest_volume, lookback_months, rank_threshold, log_detail, is_tracing):
    """
    æ£€æŸ¥ latest_volume æ˜¯å¦æ˜¯è¿‡å» N ä¸ªæœˆå†…çš„å‰ rank_threshold å
    """
    # è®¡ç®— N ä¸ªæœˆå‰çš„æ—¥æœŸ
    try:
        dt = datetime.datetime.strptime(latest_date_str, "%Y-%m-%d")
        start_date = dt - datetime.timedelta(days=lookback_months * 30)
        start_date_str = start_date.strftime("%Y-%m-%d")
    except Exception:
        return False

    # æŸ¥è¯¢è¿‡å»3ä¸ªæœˆçš„æ‰€æœ‰æˆäº¤é‡
    query = f'SELECT volume FROM "{sector_name}" WHERE name = ? AND date >= ? AND date <= ?'
    cursor.execute(query, (symbol, start_date_str, latest_date_str))
    rows = cursor.fetchall()
    
    volumes = [r[0] for r in rows if r[0] is not None]
    
    if not volumes:
        return False
        
    # æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
    sorted_volumes = sorted(volumes, reverse=True)
    
    # æˆªå–å‰ N å
    top_n_volumes = sorted_volumes[:rank_threshold]
    
    is_top_n = latest_volume in top_n_volumes
    
    # å¦ä¸€ç§åˆ¤å®šï¼šå¦‚æœæœ€æ–°æˆäº¤é‡ >= ç¬¬Nåçš„å€¼ï¼Œä¹Ÿç®—ï¼ˆå¤„ç†é‡å¤å€¼æƒ…å†µï¼‰
    # æ³¨æ„ï¼šæ•°ç»„ç´¢å¼•æ˜¯ä»0å¼€å§‹çš„ï¼Œæ‰€ä»¥ç¬¬Nåçš„ç´¢å¼•æ˜¯ rank_threshold - 1
    if len(top_n_volumes) >= rank_threshold and latest_volume >= top_n_volumes[rank_threshold - 1]:
        is_top_n = True
        
    if is_tracing:
        log_detail(f"   - [æ”¾é‡æ£€æŸ¥] è¿‡å»{lookback_months}ä¸ªæœˆè®°å½•æ•°: {len(volumes)}")
        log_detail(f"   - å‰{rank_threshold}åVol: {top_n_volumes}")
        log_detail(f"   - å½“å‰Vol:   {latest_volume}")
        log_detail(f"   - ç»“æœ: {is_top_n}")
        
    return is_top_n

def check_is_earnings_day(cursor, symbol, target_date_str):
    """
    æ£€æŸ¥ target_date_str æ˜¯å¦ä¸ºè¯¥ symbol åœ¨ Earning è¡¨ä¸­çš„æœ€æ–°è´¢æŠ¥æ—¥ã€‚
    å¦‚æœ Earning è¡¨ä¸­æœ‰è¯¥æ—¥æœŸä¸”ä¸ target_date_str åŒ¹é…ï¼Œè¿”å› Trueã€‚
    """
    try:
        # æŸ¥è¯¢è¯¥ symbol çš„æœ€æ–°ä¸€æ¡è´¢æŠ¥è®°å½•ï¼ˆæˆ–è€…ç›´æ¥æŸ¥æ˜¯å¦å­˜åœ¨è¯¥æ—¥æœŸçš„è®°å½•ï¼‰
        # è¿™é‡Œé€»è¾‘æ˜¯ï¼šå¦‚æœè¯¥æ—¥æ˜¯è´¢æŠ¥æ—¥ï¼ŒEarningè¡¨é‡Œåº”è¯¥æœ‰è¿™ä¸€å¤©çš„è®°å½•
        query = "SELECT date FROM Earning WHERE name = ? AND date = ?"
        cursor.execute(query, (symbol, target_date_str))
        row = cursor.fetchone()
        
        if row:
            return True
        return False
    except Exception as e:
        # å¦‚æœè¡¨ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å‡ºé”™ï¼Œé»˜è®¤ä¸è¿‡æ»¤
        return False

def process_condition_8(db_path, history_json_path, sector_map, target_date_override, symbol_to_trace, log_detail):
    """
    æ‰§è¡Œæ¡ä»¶8ç­–ç•¥ï¼šPE_Volume (ä¿®æ”¹ç‰ˆï¼šT-1, T-2, T-3 ä»…æ£€æŸ¥æ˜¯å¦æ”¾é‡ï¼Œä¸æ¯”è¾ƒä»·æ ¼)
    """
    log_detail("\n========== å¼€å§‹æ‰§è¡Œ æ¡ä»¶8 (PE_Volume) ç­–ç•¥ ==========")
    
    # è¯»å–é…ç½®
    rank_threshold = CONFIG.get("COND8_VOLUME_RANK_THRESHOLD", 3)
    log_detail(f"é…ç½®å‚æ•°: æ’åé˜ˆå€¼ = Top {rank_threshold}")

    # 1. ç¡®å®šåŸºå‡†æ—¥æœŸ (Today)
    base_date = target_date_override if target_date_override else datetime.date.today().isoformat()
    log_detail(f"åŸºå‡†æ—¥æœŸ (Today): {base_date}")
    candidates_volume = set()
    
    # åŠ è½½å†å²è®°å½•
    try:
        with open(history_json_path, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
    except Exception as e:
        log_detail(f"é”™è¯¯: æ— æ³•è¯»å–å†å²è®°å½•æ–‡ä»¶: {e}")
        return []

    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(db_path, timeout=60.0)
    cursor = conn.cursor()
    target_groups = CONFIG["COND8_TARGET_GROUPS"]
    
    # è·å–å¤§ç›˜åŸºå‡†æ—¥æœŸ T-1, T-2, T-3
    sample_symbol = list(sector_map.keys())[0] if sector_map else "AAPL"
    sample_sector = sector_map.get(sample_symbol, "Technology")
    
    # è·å–æœ€è¿‘ 5 å¤©æ—¥æœŸï¼Œç®—å‡º T-1, T-2, T-3
    # global_dates[0] = Today, global_dates[1] = T-1 (ä¸Šä¸€ä¸ªæœ‰æ•ˆäº¤æ˜“æ—¥)
    global_dates = get_trading_dates_list(cursor, sample_sector, sample_symbol, base_date, limit=5)
    
    if len(global_dates) < 4:
        log_detail("é”™è¯¯: æ— æ³•è·å–è¶³å¤Ÿçš„äº¤æ˜“æ—¥å†æ•°æ®ã€‚")
        conn.close()
        return []
        
    # global_dates[0] = Today
    date_t1 = global_dates[1]
    date_t2 = global_dates[2]
    date_t3 = global_dates[3]
    
    log_detail(f"ç³»ç»Ÿè®¡ç®—å‡ºçš„å…³é”®æ—¥æœŸ: T-1={date_t1}, T-2={date_t2}, T-3={date_t3}")
    
    # å®šä¹‰ä»»åŠ¡åˆ—è¡¨
    # æ ¼å¼: (Target_Date_In_History, Date_Index_In_List, Task_Name)
    # Date_Index_In_List: 1ä»£è¡¨T-1, 2ä»£è¡¨T-2, 3ä»£è¡¨T-3
    tasks = [
        (date_t1, 1, "T-1ç­–ç•¥"),
        (date_t2, 2, "T-2ç­–ç•¥"),
        (date_t3, 3, "T-3ç­–ç•¥")
    ]
    
    for hist_date, date_idx, task_name in tasks:
        # 1. ä»å†å²æ–‡ä»¶ä¸­æå–è¯¥æ—¥æœŸçš„æ‰€æœ‰ symbol
        symbols_on_date = set()
        for group in target_groups:
            grp_data = history_data.get(group, {})
            if isinstance(grp_data, dict):
                syms = grp_data.get(hist_date, [])
                symbols_on_date.update(syms)
        
        symbols_on_date = sorted(list(symbols_on_date))
        if symbol_to_trace:
            log_detail(f" -> æ­£åœ¨æ‰«æ {task_name} (æ—¥æœŸ: {hist_date})ï¼ŒåŒ…å« {len(symbols_on_date)} ä¸ªå€™é€‰ã€‚")
            if symbol_to_trace in symbols_on_date:
                log_detail(f"    !!! ç›®æ ‡ {symbol_to_trace} åœ¨ {hist_date} çš„å†å²è®°å½•ä¸­ï¼Œå¼€å§‹æ£€æŸ¥...")
        
        for symbol in symbols_on_date:
            is_tracing = (symbol == symbol_to_trace)
            sector = sector_map.get(symbol)
            if not sector: continue
            
            # è·å–è¯¥è‚¡çš„å…·ä½“äº¤æ˜“æ—¥å†
            # è·å– 5 å¤©: Today(0), T-1(1), T-2(2), T-3(3)
            dates = get_trading_dates_list(cursor, sector, symbol, base_date, limit=5)
            
            # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            if len(dates) < 4: 
                if is_tracing: log_detail(f"    x [å¤±è´¥] äº¤æ˜“æ•°æ®ä¸è¶³ (ä»…{len(dates)}å¤©)")
                continue
            
            # ç¡®è®¤æ—¥æœŸå¯¹é½ï¼šä¸ªè‚¡çš„ dates[date_idx] åº”è¯¥æ˜¯ hist_date
            # ä¾‹å¦‚ T-2 ç­–ç•¥ï¼Œdates[2] å¿…é¡»ç­‰äº hist_date
            if dates[date_idx] != hist_date:
                if is_tracing: log_detail(f"    x [è·³è¿‡] ä¸ªè‚¡{task_name}æ—¥æœŸ({dates[date_idx]}) ä¸ ä»»åŠ¡æ—¥æœŸ({hist_date}) ä¸ä¸€è‡´")
                continue
            
            # è·å–ä»Šæ—¥ (Today) çš„æˆäº¤é‡
            # åªéœ€è¦æŸ¥è¯¢ dates[0] (Today) çš„æ•°æ®
            query = f'SELECT volume FROM "{sector}" WHERE name = ? AND date = ?'
            cursor.execute(query, (symbol, dates[0]))
            row = cursor.fetchone()
            
            if not row:
                if is_tracing: log_detail(f"    x [å¤±è´¥] ç¼ºå°‘ä»Šæ—¥({dates[0]})æ•°æ®ã€‚")
                continue
            
            today_volume = row[0]
            if today_volume is None: continue

            # æ ¸å¿ƒåˆ¤æ–­é€»è¾‘ï¼šåªæ£€æŸ¥æ”¾é‡ (ä½¿ç”¨é…ç½®çš„ rank_threshold)
            vol_cond = check_volume_rank(
                cursor, sector, symbol, dates[0], today_volume, 
                CONFIG["COND8_VOLUME_LOOKBACK_MONTHS"], 
                rank_threshold, # ä¼ å…¥é…ç½®çš„é˜ˆå€¼
                log_detail, is_tracing
            )
            
            if vol_cond:
                # ================== è´¢æŠ¥æ—¥è¿‡æ»¤é€»è¾‘ ==================
                # æ£€æŸ¥ä»Šæ—¥(dates[0])æ˜¯å¦ä¸ºè´¢æŠ¥æ—¥
                if check_is_earnings_day(cursor, symbol, dates[0]):
                    if is_tracing: log_detail(f"    ğŸ›‘ [è¿‡æ»¤] ä»Šæ—¥({dates[0]}) ä¸ºè´¢æŠ¥æ—¥ï¼Œå‰”é™¤ã€‚")
                    continue
                # =================================================

                candidates_volume.add(symbol)
                if is_tracing: log_detail(f"    âœ… [é€šè¿‡] {task_name} æ”¾é‡æ¡ä»¶æ»¡è¶³ï¼")

    conn.close()
    
    result_list = sorted(list(candidates_volume))
    log_detail(f"æ¡ä»¶8 (PE_Volume) ç­›é€‰å®Œæˆï¼Œå…±å‘½ä¸­ {len(result_list)} ä¸ª: {result_list}")
    return result_list

# --- 4. ä¸»æ‰§è¡Œæµç¨‹ ---

def run_pe_volume_logic(log_detail):
    log_detail("PE_Volume ç‹¬ç«‹ç¨‹åºå¼€å§‹è¿è¡Œ...")
    if SYMBOL_TO_TRACE: log_detail(f"å½“å‰è¿½è¸ªçš„ SYMBOL: {SYMBOL_TO_TRACE}")
    if TARGET_DATE:
        log_detail(f"\nâš ï¸âš ï¸âš ï¸ æ³¨æ„ï¼šå½“å‰å¤„äºã€å›æµ‹æ¨¡å¼ã€‘ï¼Œç›®æ ‡æ—¥æœŸï¼š{TARGET_DATE} âš ï¸âš ï¸âš ï¸")
        log_detail("æœ¬æ¬¡è¿è¡Œå°†ã€ä¸ä¼šã€‘æ›´æ–° Panel å’Œ History JSON æ–‡ä»¶ã€‚")
    
    # 1. åŠ è½½é…ç½®å’Œæ˜ å°„
    tag_blacklist, hot_tags = load_tag_settings(TAGS_SETTING_JSON_FILE)
    symbol_to_sector_map = load_all_symbols(SECTORS_JSON_FILE, CONFIG["TARGET_SECTORS"])
    symbol_to_tags_map = load_symbol_tags(DESCRIPTION_JSON_FILE)

    if not symbol_to_sector_map:
        log_detail("é”™è¯¯: æ— æ³•åŠ è½½æ¿å—æ˜ å°„ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    # 2. æ‰§è¡Œæ ¸å¿ƒç­–ç•¥
    raw_pe_volume = process_condition_8(
        DB_FILE, 
        EARNING_HISTORY_JSON_FILE, 
        symbol_to_sector_map, 
        TARGET_DATE, 
        SYMBOL_TO_TRACE, 
        log_detail
    )

    # 3. è¿‡æ»¤ Tag é»‘åå• (å·²ä¿®æ”¹ï¼šä¸å†è¿‡æ»¤ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç»“æœ)
    # def filter_tags(syms):
    #     return [s for s in syms if not set(symbol_to_tags_map.get(s, [])).intersection(tag_blacklist)]
    
    # final_pe_volume_to_write = sorted(list(set(filter_tags(raw_pe_volume))))
    
    # ç›´æ¥èµ‹å€¼ï¼Œä¸è¿‡æ»¤é»‘åå•Tag
    final_pe_volume_to_write = sorted(list(set(raw_pe_volume)))
    
    # 4. æ„å»ºå¤‡æ³¨ (Note) - ä¾ç„¶ä¿ç•™çƒ­ç‚¹æ ‡è®°é€»è¾‘
    def build_symbol_note_map(symbols):
        note_map = {}
        for sym in symbols:
            tags = set(symbol_to_tags_map.get(sym, []))
            is_hot = bool(tags & hot_tags)
            note_map[sym] = f"{sym}çƒ­" if is_hot else ""
        return note_map
        
    pe_volume_notes = build_symbol_note_map(final_pe_volume_to_write)

    # 5. å›æµ‹å®‰å…¨æ‹¦æˆª
    if TARGET_DATE:
        log_detail("\n" + "="*60)
        log_detail(f"ğŸ›‘ [å®‰å…¨æ‹¦æˆª] å›æµ‹æ¨¡å¼ (Date: {TARGET_DATE}) å·²å¯ç”¨ã€‚")
        log_detail(f"ğŸ“Š [æ¨¡æ‹Ÿç»“æœ] PE_Volume å‘½ä¸­æ•°é‡: {len(final_pe_volume_to_write)} ä¸ª")
        log_detail(f"   åˆ—è¡¨: {final_pe_volume_to_write}")
        if SYMBOL_TO_TRACE:
            in_list = SYMBOL_TO_TRACE in final_pe_volume_to_write
            log_detail(f"ğŸ” [éªŒè¯] Symbol '{SYMBOL_TO_TRACE}' æ˜¯å¦å‘½ä¸­: {in_list}")
        log_detail("="*60 + "\n")
        return

    # 6. å†™å…¥ Panel
    log_detail(f"\næ­£åœ¨å†™å…¥ Panel æ–‡ä»¶... (æ•°é‡: {len(final_pe_volume_to_write)})")
    update_json_panel(final_pe_volume_to_write, PANEL_JSON_FILE, 'PE_Volume', symbol_to_note=pe_volume_notes)
    update_json_panel(final_pe_volume_to_write, PANEL_JSON_FILE, 'PE_Volume_backup', symbol_to_note=pe_volume_notes)

    # 7. å†™å…¥ History (Raw Data)
    log_detail(f"æ­£åœ¨æ›´æ–° History æ–‡ä»¶... (Raw æ•°é‡: {len(raw_pe_volume)})")
    update_earning_history_json(EARNING_HISTORY_JSON_FILE, "PE_Volume", raw_pe_volume, log_detail)

    log_detail("ç¨‹åºè¿è¡Œç»“æŸã€‚")

def main():
    if SYMBOL_TO_TRACE:
        print(f"è¿½è¸ªæ¨¡å¼å·²å¯ç”¨ï¼Œç›®æ ‡: {SYMBOL_TO_TRACE}ã€‚æ—¥å¿—å°†å†™å…¥: {LOG_FILE_PATH}")
        try:
            with open(LOG_FILE_PATH, 'w', encoding='utf-8') as log_file:
                def log_detail_file(message):
                    log_file.write(message + '\n')
                    print(message)
                run_pe_volume_logic(log_detail_file)
        except IOError as e:
            print(f"é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æˆ–å†™å…¥æ—¥å¿—æ–‡ä»¶ {LOG_FILE_PATH}: {e}")
    else:
        print("è¿½è¸ªæ¨¡å¼æœªå¯ç”¨ã€‚æ—¥å¿—ä»…è¾“å‡ºåˆ°æ§åˆ¶å°ã€‚")
        def log_detail_console(message):
            print(message)
        run_pe_volume_logic(log_detail_console)

if __name__ == '__main__':
    main()