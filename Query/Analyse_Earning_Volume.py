import json
import sqlite3
import os
import datetime

USER_HOME = os.path.expanduser("~")
BASE_CODING_DIR = os.path.join(USER_HOME, "Coding")

# --- 1. é…ç½®æ–‡ä»¶å’Œè·¯å¾„ ---
BASE_PATH = USER_HOME

# ================= é…ç½®åŒºåŸŸ =================
# å¦‚æœä¸ºç©ºï¼Œåˆ™è¿è¡Œâ€œä»Šå¤©â€æ¨¡å¼ï¼›å¦‚æœå¡«å…¥æ—¥æœŸï¼ˆå¦‚ "2024-11-03"ï¼‰ï¼Œåˆ™è¿è¡Œå›æµ‹æ¨¡å¼
SYMBOL_TO_TRACE = "" 
TARGET_DATE = ""

# SYMBOL_TO_TRACE = "IESC"
# TARGET_DATE = "2026-01-09"

# 3. æ—¥å¿—è·¯å¾„
LOG_FILE_PATH = os.path.join(BASE_PATH, "Downloads", "PE_Volume_trace_log.txt")

PATHS = {
    "config_dir": os.path.join(BASE_CODING_DIR, 'Financial_System', 'Modules'),
    "db_dir": os.path.join(BASE_CODING_DIR, 'Database'),
    "sectors_json": lambda config_dir: os.path.join(config_dir, 'Sectors_All.json'),
    "panel_json": lambda config_dir: os.path.join(config_dir, 'Sectors_panel.json'),
    "description_json": lambda config_dir: os.path.join(config_dir, 'description.json'),
    "tags_setting_json": lambda config_dir: os.path.join(config_dir, 'tags_filter.json'),
    "earnings_history_json": lambda config_dir: os.path.join(config_dir, 'Earning_History.json'),
    "db_file": lambda db_dir: os.path.join(db_dir, 'Finance.db'),
}

# åŠ¨æ€ç”Ÿæˆå®Œæ•´è·¯å¾„
CONFIG_DIR = PATHS["config_dir"]
DB_DIR = PATHS["db_dir"]
DB_FILE = PATHS["db_file"](DB_DIR)
SECTORS_JSON_FILE = PATHS["sectors_json"](CONFIG_DIR)
PANEL_JSON_FILE = PATHS["panel_json"](CONFIG_DIR)
DESCRIPTION_JSON_FILE = PATHS["description_json"](CONFIG_DIR)
TAGS_SETTING_JSON_FILE = PATHS["tags_setting_json"](CONFIG_DIR)
EARNING_HISTORY_JSON_FILE = PATHS["earnings_history_json"](CONFIG_DIR)

CONFIG = {
    "TARGET_SECTORS": {
        "Basic_Materials", "Communication_Services", "Consumer_Cyclical",
        "Consumer_Defensive", "Energy", "Financial_Services", "Healthcare",
        "Industrials", "Real_Estate", "Technology", "Utilities"
    },
    # ========== ç›®æ ‡åˆ†ç»„ (ä¸¤ä¸ªç­–ç•¥å…±ç”¨) ==========
    "TARGET_GROUPS": [
        "OverSell_W", "PE_Deeper", "PE_Deep", 
        "PE_W", "PE_valid", "PE_invalid", "season", "no_season"
    ],
    # ========== ç­–ç•¥1 (PE_Volume) å‚æ•° ==========
    "COND8_VOLUME_LOOKBACK_MONTHS": 3,   # è¿‡å»3ä¸ªæœˆ
    "COND8_VOLUME_RANK_THRESHOLD": 4,    # æˆäº¤é‡æ’åå‰ N å (é»˜è®¤3ï¼Œä»£ç é€»è¾‘æ˜¯ <4)
    
    # ========== ç­–ç•¥2 (PE_Volume_up) å‚æ•° ==========
    "COND_UP_HISTORY_LOOKBACK_DAYS": 5,  # å†å²è®°å½•å›æº¯å¤©æ•°
    "COND_UP_VOL_RANK_MONTHS": 1,        # æ”¾é‡æ£€æŸ¥å›æº¯æœˆä»½ (1ä¸ªæœˆ)
    "COND_UP_VOL_RANK_THRESHOLD": 3,     # æ”¾é‡æ£€æŸ¥å‰ N å
}

# --- 2. è¾…åŠ©ä¸æ–‡ä»¶æ“ä½œæ¨¡å— ---

def load_tag_settings(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            settings = json.load(f)
        tag_blacklist = set(settings.get('BLACKLIST_TAGS', []))
        hot_tags = set(settings.get('HOT_TAGS', []))
        return tag_blacklist, hot_tags
    except Exception:
        return set(), set()

def load_all_symbols(json_path, target_sectors):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            all_sectors_data = json.load(f)
        symbol_to_sector_map = {}
        for sector, symbols in all_sectors_data.items():
            if sector in target_sectors:
                for symbol in symbols:
                    symbol_to_sector_map[symbol] = sector
        return symbol_to_sector_map
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½symbolså¤±è´¥: {e}")
        return None

def load_symbol_tags(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        symbol_tag_map = {}
        stock_list = data.get('stocks', [])
        for item in stock_list:
            symbol = item.get('symbol')
            tags = item.get('tag', [])
            if symbol:
                symbol_tag_map[symbol] = tags
        return symbol_tag_map
    except Exception:
        return {}

def update_json_panel(symbols_list, json_path, group_name, symbol_to_note=None):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {}

    if symbol_to_note is None:
        data[group_name] = {symbol: "" for symbol in sorted(symbols_list)}
    else:
        data[group_name] = {symbol: symbol_to_note.get(symbol, "") for symbol in sorted(symbols_list)}

    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"é”™è¯¯: å†™å…¥JSONæ–‡ä»¶å¤±è´¥: {e}")

def update_earning_history_json(file_path, group_name, symbols_to_add, log_detail, base_date_str):
    log_detail(f"\n--- æ›´æ–°å†å²è®°å½•æ–‡ä»¶: {os.path.basename(file_path)} -> '{group_name}' ---")
    
    # ä½¿ç”¨ä¼ å…¥çš„åŸºå‡†æ—¥æœŸä½œä¸ºè®°å½•æ—¥æœŸ
    record_date_str = base_date_str

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {}

    if group_name not in data:
        data[group_name] = {}

    existing_symbols = data[group_name].get(record_date_str, [])
    combined_symbols = set(existing_symbols) | set(symbols_to_add)
    updated_symbols = sorted(list(combined_symbols))
    
    data[group_name][record_date_str] = updated_symbols
    num_added = len(updated_symbols) - len(existing_symbols)

    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        log_detail(f"æˆåŠŸæ›´æ–°å†å²è®°å½•ã€‚æ—¥æœŸ: {record_date_str}, åˆ†ç»„: '{group_name}'.")
        log_detail(f" - æœ¬æ¬¡æ–°å¢ {num_added} ä¸ªä¸é‡å¤çš„ symbolã€‚")
    except Exception as e:
        log_detail(f"é”™è¯¯: å†™å…¥å†å²è®°å½•æ–‡ä»¶å¤±è´¥: {e}")

# --- 3. æ ¸å¿ƒé€»è¾‘æ¨¡å— ---

def get_trading_dates_list(cursor, sector_name, symbol, end_date_str, limit=10):
    """
    è·å–åŒ…å« end_date_str åœ¨å†…çš„æœ€è¿‘ limit ä¸ªäº¤æ˜“æ—¥æ—¥æœŸåˆ—è¡¨ã€‚
    è¿”å›: ['2025-01-28', '2025-01-27', '2025-01-24', ...] (å€’åº)
    """
    query = f'SELECT date FROM "{sector_name}" WHERE name = ? AND date <= ? ORDER BY date DESC LIMIT ?'
    cursor.execute(query, (symbol, end_date_str, limit))
    rows = cursor.fetchall()
    return [r[0] for r in rows]

def check_volume_rank(cursor, sector_name, symbol, latest_date_str, latest_volume, lookback_months, rank_threshold, log_detail, is_tracing):
    """
    é€šç”¨æ£€æŸ¥ï¼šlatest_volume æ˜¯å¦æ˜¯è¿‡å» lookback_months ä¸ªæœˆå†…çš„å‰ rank_threshold å
    """
    # è®¡ç®— N ä¸ªæœˆå‰çš„æ—¥æœŸ
    try:
        dt = datetime.datetime.strptime(latest_date_str, "%Y-%m-%d")
        start_date = dt - datetime.timedelta(days=lookback_months * 30)
        start_date_str = start_date.strftime("%Y-%m-%d")
    except Exception:
        return False

    # æŸ¥è¯¢è¿‡å» N ä¸ªæœˆçš„æ‰€æœ‰æ—¥æœŸå’Œæˆäº¤é‡
    query = f'SELECT date, volume FROM "{sector_name}" WHERE name = ? AND date >= ? AND date <= ?'
    cursor.execute(query, (symbol, start_date_str, latest_date_str))
    rows = cursor.fetchall() # ç»“æœæ˜¯ [(date1, vol1), (date2, vol2), ...]
    
    # è¿‡æ»¤æ‰ None å€¼
    valid_data = [(r[0], r[1]) for r in rows if r[1] is not None]
    
    if not valid_data:
        return False
        
    # æ’åºï¼šæŒ‰ volume (x[1]) ä»å¤§åˆ°å°æ’
    sorted_data = sorted(valid_data, key=lambda x: x[1], reverse=True)
    
    # æˆªå–å‰ N å
    top_n_data = sorted_data[:rank_threshold]
    
    # æå–å‰ N åçš„æˆäº¤é‡æ•°å€¼
    top_n_volumes = [item[1] for item in top_n_data]
    
    # åˆ¤å®šé€»è¾‘ï¼šå½“å‰æˆäº¤é‡æ˜¯å¦åœ¨å‰ N åä¸­ï¼Œæˆ–è€…å¤§äºç­‰äºç¬¬ N åçš„å€¼
    is_top_n = False
    if latest_volume in top_n_volumes:
        is_top_n = True
    elif len(top_n_volumes) >= rank_threshold and latest_volume >= top_n_volumes[rank_threshold - 1]:
        is_top_n = True
        
    if is_tracing:
        log_detail(f"   - [Rankæ£€æŸ¥] å›æº¯{lookback_months}ä¸ªæœˆ, è®°å½•æ•°: {len(valid_data)}")
        top_n_str = ", ".join([f"[{d}]: {v}" for d, v in top_n_data])
        log_detail(f"   - å‰{rank_threshold}å: {top_n_str}")
        log_detail(f"   - å½“å‰é‡: {latest_volume} -> ç»“æœ: {is_top_n}")
        
    return is_top_n

def check_is_earnings_day(cursor, symbol, target_date_str):
    """
    æ£€æŸ¥ target_date_str æ˜¯å¦ä¸ºè¯¥ symbol åœ¨ Earning è¡¨ä¸­çš„æœ€æ–°è´¢æŠ¥æ—¥ã€‚
    """
    try:
        # æŸ¥è¯¢è¯¥ symbol çš„æœ€æ–°ä¸€æ¡è´¢æŠ¥è®°å½•ï¼ˆæˆ–è€…ç›´æ¥æŸ¥æ˜¯å¦å­˜åœ¨è¯¥æ—¥æœŸçš„è®°å½•ï¼‰
        # è¿™é‡Œé€»è¾‘æ˜¯ï¼šå¦‚æœè¯¥æ—¥æ˜¯è´¢æŠ¥æ—¥ï¼ŒEarningè¡¨é‡Œåº”è¯¥æœ‰è¿™ä¸€å¤©çš„è®°å½•
        query = "SELECT date FROM Earning WHERE name = ? AND date = ?"
        cursor.execute(query, (symbol, target_date_str))
        row = cursor.fetchone()
        if row:
            return True
        return False
    except Exception as e:
        # å¦‚æœè¡¨ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å‡ºé”™ï¼Œé»˜è®¤ä¸è¿‡æ»¤
        return False

# --- ç­–ç•¥1: PE_Volume (T, T-1, T-2, T-3 æ”¾é‡) ---
def process_condition_8(db_path, history_json_path, sector_map, target_date_override, symbol_to_trace, log_detail):
    """
    æ‰§è¡Œæ¡ä»¶8ç­–ç•¥ï¼šPE_Volume (ä¿®æ”¹ç‰ˆï¼šT-1, T-2, T-3 ä»…æ£€æŸ¥æ˜¯å¦æ”¾é‡ï¼Œä¸æ¯”è¾ƒä»·æ ¼)
    """
    log_detail("\n========== å¼€å§‹æ‰§è¡Œ æ¡ä»¶8 (PE_Volume) ç­–ç•¥ ==========")
    
    # è¯»å–é…ç½®
    rank_threshold = CONFIG.get("COND8_VOLUME_RANK_THRESHOLD", 3)
    lookback_months = CONFIG.get("COND8_VOLUME_LOOKBACK_MONTHS", 3)
    log_detail(f"é…ç½®å‚æ•°: æ’åé˜ˆå€¼ = Top {rank_threshold}")

    # 1. ç¡®å®šåŸºå‡†æ—¥æœŸ (Today)
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œåˆ™è·å–æ˜¨å¤©çš„æ—¥æœŸ
    base_date = target_date_override if target_date_override else (datetime.date.today() - datetime.timedelta(days=1)).isoformat()
    log_detail(f"åŸºå‡†æ—¥æœŸ (Today): {base_date}")
    candidates_volume = set()
    
    # åŠ è½½å†å²è®°å½•
    try:
        with open(history_json_path, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
    except Exception as e:
        log_detail(f"é”™è¯¯: æ— æ³•è¯»å–å†å²è®°å½•æ–‡ä»¶: {e}")
        return []

    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(db_path, timeout=60.0)
    cursor = conn.cursor()
    target_groups = CONFIG["TARGET_GROUPS"]
    
    # è·å–å¤§ç›˜åŸºå‡†æ—¥æœŸ T-1, T-2, T-3
    sample_symbol = list(sector_map.keys())[0] if sector_map else "AAPL"
    sample_sector = sector_map.get(sample_symbol, "Technology")
    
    # è·å–æœ€è¿‘ 5 å¤©æ—¥æœŸï¼Œç®—å‡º T-1, T-2, T-3
    # global_dates[0] = Today, global_dates[1] = T-1 (ä¸Šä¸€ä¸ªæœ‰æ•ˆäº¤æ˜“æ—¥)
    global_dates = get_trading_dates_list(cursor, sample_sector, sample_symbol, base_date, limit=5)
    
    if len(global_dates) < 4:
        log_detail("é”™è¯¯: æ— æ³•è·å–è¶³å¤Ÿçš„äº¤æ˜“æ—¥å†æ•°æ®ã€‚")
        conn.close()
        return []
        
    # å®šä¹‰å…³é”®æ—¥æœŸ
    date_t0 = global_dates[0] # Today
    date_t1 = global_dates[1]
    date_t2 = global_dates[2]
    date_t3 = global_dates[3]
    
    log_detail(f"ç³»ç»Ÿè®¡ç®—å‡ºçš„å…³é”®æ—¥æœŸ: T={date_t0}, T-1={date_t1}, T-2={date_t2}, T-3={date_t3}")
    
    # å®šä¹‰ä»»åŠ¡åˆ—è¡¨
    # æ ¼å¼: (Target_Date_In_History, Date_Index_In_List, Task_Name)
    # Date_Index_In_List: 1ä»£è¡¨T-1, 2ä»£è¡¨T-2, 3ä»£è¡¨T-3
    tasks = [
        (date_t0, 0, "Tç­–ç•¥"),
        (date_t1, 1, "T-1ç­–ç•¥"),
        (date_t2, 2, "T-2ç­–ç•¥"),
        (date_t3, 3, "T-3ç­–ç•¥")
    ]
    
    for hist_date, date_idx, task_name in tasks:
        # 1. ä»å†å²æ–‡ä»¶ä¸­æå–è¯¥æ—¥æœŸçš„æ‰€æœ‰ symbol
        symbols_on_date = set()
        for group in target_groups:
            grp_data = history_data.get(group, {})
            if isinstance(grp_data, dict):
                syms = grp_data.get(hist_date, [])
                symbols_on_date.update(syms)
        
        symbols_on_date = sorted(list(symbols_on_date))
        log_detail(f" -> æ­£åœ¨æ‰«æ {task_name} (æ—¥æœŸ: {hist_date})ï¼ŒåŒ…å« {len(symbols_on_date)} ä¸ªå€™é€‰ã€‚")
        if symbol_to_trace:
            if symbol_to_trace in symbols_on_date:
                log_detail(f"    !!! ç›®æ ‡ {symbol_to_trace} åœ¨ {hist_date} çš„å†å²è®°å½•ä¸­ï¼Œå¼€å§‹æ£€æŸ¥...")
        
        for symbol in symbols_on_date:
            is_tracing = (symbol == symbol_to_trace)
            sector = sector_map.get(symbol)
            if not sector: continue
            
            # è·å–è¯¥è‚¡çš„å…·ä½“äº¤æ˜“æ—¥å†
            # è·å– 5 å¤©: Today(0), T-1(1), T-2(2), T-3(3)
            dates = get_trading_dates_list(cursor, sector, symbol, base_date, limit=5)
            
            # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            if len(dates) < 4: 
                if is_tracing: log_detail(f"    x [å¤±è´¥] äº¤æ˜“æ•°æ®ä¸è¶³ (ä»…{len(dates)}å¤©)")
                continue
            
            # ç¡®è®¤æ—¥æœŸå¯¹é½ï¼šä¸ªè‚¡çš„ dates[date_idx] åº”è¯¥æ˜¯ hist_date
            # ä¾‹å¦‚ T-2 ç­–ç•¥ï¼Œdates[2] å¿…é¡»ç­‰äº hist_date
            if dates[date_idx] != hist_date:
                if is_tracing: log_detail(f"    x [è·³è¿‡] ä¸ªè‚¡{task_name}æ—¥æœŸ({dates[date_idx]}) ä¸ ä»»åŠ¡æ—¥æœŸ({hist_date}) ä¸ä¸€è‡´")
                continue
            
            # è·å–ä»Šæ—¥ (Today) çš„æˆäº¤é‡
            # åªéœ€è¦æŸ¥è¯¢ dates[0] (Today) çš„æ•°æ®
            query = f'SELECT volume FROM "{sector}" WHERE name = ? AND date = ?'
            cursor.execute(query, (symbol, dates[0]))
            row = cursor.fetchone()
            
            if not row:
                if is_tracing: log_detail(f"    x [å¤±è´¥] ç¼ºå°‘ä»Šæ—¥({dates[0]})æ•°æ®ã€‚")
                continue
            
            today_volume = row[0]
            if today_volume is None: continue

            # æ ¸å¿ƒåˆ¤æ–­é€»è¾‘ï¼šåªæ£€æŸ¥æ”¾é‡ (ä½¿ç”¨é…ç½®çš„ rank_threshold)
            vol_cond = check_volume_rank(
                cursor, sector, symbol, dates[0], today_volume, 
                CONFIG["COND8_VOLUME_LOOKBACK_MONTHS"], 
                rank_threshold, # ä¼ å…¥é…ç½®çš„é˜ˆå€¼
                log_detail, is_tracing
            )
            
            if vol_cond:
                # ================== è´¢æŠ¥æ—¥è¿‡æ»¤é€»è¾‘ ==================
                # æ£€æŸ¥ä»Šæ—¥(dates[0])æ˜¯å¦ä¸ºè´¢æŠ¥æ—¥
                if check_is_earnings_day(cursor, symbol, dates[0]):
                    if is_tracing: log_detail(f"    ğŸ›‘ [è¿‡æ»¤] ä»Šæ—¥({dates[0]}) ä¸ºè´¢æŠ¥æ—¥ï¼Œå‰”é™¤ã€‚")
                    continue

                candidates_volume.add(symbol)
                if is_tracing: log_detail(f"    âœ… [é€šè¿‡] {task_name} æ”¾é‡æ¡ä»¶æ»¡è¶³ï¼")

    conn.close()
    
    result_list = sorted(list(candidates_volume))
    log_detail(f"æ¡ä»¶8 (PE_Volume) ç­›é€‰å®Œæˆï¼Œå…±å‘½ä¸­ {len(result_list)} ä¸ª: {result_list}")
    return result_list

# --- ç­–ç•¥2: PE_Volume_up (5å¤©å†…å‡ºç° + æ”¾é‡ä¸Šæ¶¨/ç¼©é‡ä¸Šæ¶¨) ---
def process_pe_volume_up(db_path, history_json_path, sector_map, target_date_override, symbol_to_trace, log_detail):
    log_detail("\n========== å¼€å§‹æ‰§è¡Œ ç­–ç•¥2 (PE_Volume_up) ==========")
    
    # é…ç½®å‚æ•°
    lookback_days = CONFIG.get("COND_UP_HISTORY_LOOKBACK_DAYS", 5)
    vol_rank_months = CONFIG.get("COND_UP_VOL_RANK_MONTHS", 1)
    vol_rank_threshold = CONFIG.get("COND_UP_VOL_RANK_THRESHOLD", 3)
    
    # ã€å›æµ‹é€»è¾‘ã€‘è¿™é‡Œå¤„ç†å›æµ‹æ—¥æœŸ
    # å¦‚æœ target_date_override å­˜åœ¨ï¼Œåˆ™æ‰€æœ‰é€»è¾‘éƒ½åŸºäºè¿™ä¸ªæ—¥æœŸ
    base_date = target_date_override if target_date_override else (datetime.date.today() - datetime.timedelta(days=1)).isoformat()
    
    # 1. è¿æ¥æ•°æ®åº“è·å–å…¨å±€æ—¥æœŸ
    conn = sqlite3.connect(db_path, timeout=60.0)
    cursor = conn.cursor()
    
    sample_symbol = list(sector_map.keys())[0] if sector_map else "AAPL"
    sample_sector = sector_map.get(sample_symbol, "Technology")
    # è·å–æœ€è¿‘5ä¸ªäº¤æ˜“æ—¥ (T, T-1, T-2, T-3, T-4)
    global_dates = get_trading_dates_list(cursor, sample_sector, sample_symbol, base_date, limit=lookback_days)
    
    if len(global_dates) < 2:
        log_detail("é”™è¯¯: äº¤æ˜“æ—¥æ•°æ®ä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œç­–ç•¥2ã€‚")
        conn.close()
        return []
    
    log_detail(f"æ‰«æå†å²æ—¥æœŸèŒƒå›´ (æˆªæ­¢ {base_date}): {global_dates}")

    # 2. ä»Historyä¸­æ”¶é›†å€™é€‰è‚¡
    try:
        with open(history_json_path, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
    except Exception:
        conn.close()
        return []

    target_groups = CONFIG["TARGET_GROUPS"]
    candidate_symbols = set()
    
    for hist_date in global_dates:
        for group in target_groups:
            grp_data = history_data.get(group, {})
            if isinstance(grp_data, dict):
                syms = grp_data.get(hist_date, [])
                candidate_symbols.update(syms)
    
    candidate_symbols = sorted(list(candidate_symbols))
    log_detail(f"5å¤©å†…å†å²è®°å½•å…±æ‰«æåˆ° {len(candidate_symbols)} ä¸ªå€™é€‰ Symbolã€‚")

    results = []
    
    # 3. é€ä¸ªæ£€æŸ¥é€»è¾‘
    for symbol in candidate_symbols:
        is_tracing = (symbol == symbol_to_trace)
        sector = sector_map.get(symbol)
        if not sector: continue
        
        if is_tracing: log_detail(f"--- æ­£åœ¨æ£€æŸ¥ {symbol} (ç­–ç•¥2) ---")

        # ã€å›æµ‹é€»è¾‘ã€‘è·å–è¯¥è‚¡æœ€è¿‘2å¤©çš„æ•°æ® (æœ€æ–°, æ¬¡æ–°)
        # å…³é”®ç‚¹ï¼šWHERE date <= base_dateï¼Œç¡®ä¿ä¸è¯»å–æœªæ¥çš„æ•°æ®
        query = f'SELECT date, price, volume FROM "{sector}" WHERE name = ? AND date <= ? ORDER BY date DESC LIMIT 2'
        cursor.execute(query, (symbol, base_date))
        rows = cursor.fetchall()
        
        if len(rows) < 2:
            if is_tracing: log_detail(f"    x æ•°æ®ä¸è¶³2å¤©ï¼Œè·³è¿‡ã€‚")
            continue
            
        # rows[0] = Latest (T), rows[1] = Previous (T-1)
        # åœ¨å›æµ‹æ¨¡å¼ä¸‹ï¼ŒLatest å°±æ˜¯å›æµ‹çš„ç›®æ ‡æ—¥æœŸ
        date_curr, price_curr, vol_curr = rows[0]
        date_prev, price_prev, vol_prev = rows[1]
        
        if price_curr is None or price_prev is None or vol_curr is None or vol_prev is None:
            continue

        # ç‰¹å¾1: å¿…é¡»ä¸Šæ¶¨ (æœ€æ–°ä»· > æ¬¡æ–°ä»·)
        if price_curr <= price_prev:
            if is_tracing: log_detail(f"    x ä»·æ ¼æœªä¸Šæ¶¨ ({price_curr} <= {price_prev})ï¼Œè·³è¿‡ã€‚")
            continue
            
        # è´¢æŠ¥æ—¥è¿‡æ»¤ (å¯é€‰ï¼Œä¿æŒç³»ç»Ÿä¸€è‡´æ€§)
        if check_is_earnings_day(cursor, symbol, date_curr):
            if is_tracing: log_detail(f"    ğŸ›‘ ä»Šæ—¥æ˜¯è´¢æŠ¥æ—¥ï¼Œè·³è¿‡ã€‚")
            continue

        is_match = False
        reason = ""

        # ç‰¹å¾2: æ£€æŸ¥æˆäº¤é‡å…³ç³»
        if vol_curr > vol_prev:
            # === æ”¾é‡ä¸Šæ¶¨ ===
            # é¢å¤–æ¡ä»¶: æœ€æ–°é‡åœ¨è¿‡å»1ä¸ªæœˆå†…æ’åå‰3
            is_top_vol = check_volume_rank(
                cursor, sector, symbol, date_curr, vol_curr, 
                vol_rank_months, vol_rank_threshold, log_detail, is_tracing
            )
            if is_top_vol:
                is_match = True
                reason = "æ”¾é‡ä¸Šæ¶¨ (Top Vol)"
            else:
                if is_tracing: log_detail(f"    x æ”¾é‡ä½†æœªè¿›å…¥å‰{vol_rank_threshold}åã€‚")
        else:
            # === ç¼©é‡ä¸Šæ¶¨ ===
            # æ¡ä»¶: é‡ç¼© (vol_curr < vol_prev) ä¸” ä»·æ¶¨ (å·²æ»¡è¶³)
            is_match = True
            reason = "ç¼©é‡ä¸Šæ¶¨"
            
        if is_match:
            results.append(symbol)
            if is_tracing: log_detail(f"    âœ… [é€‰ä¸­] {reason} (P:{price_curr}>{price_prev}, V:{vol_curr} vs {vol_prev})")

    conn.close()
    log_detail(f"ç­–ç•¥2 (PE_Volume_up) ç­›é€‰å®Œæˆï¼Œå…±å‘½ä¸­ {len(results)} ä¸ªã€‚")
    return sorted(results)

# --- 4. ä¸»æ‰§è¡Œæµç¨‹ ---

def run_pe_volume_logic(log_detail):
    log_detail("PE_Volume åŒç­–ç•¥ç¨‹åºå¼€å§‹è¿è¡Œ...")
    if SYMBOL_TO_TRACE: log_detail(f"å½“å‰è¿½è¸ªçš„ SYMBOL: {SYMBOL_TO_TRACE}")
    
    base_date_str = TARGET_DATE if TARGET_DATE else (datetime.date.today() - datetime.timedelta(days=1)).isoformat()

    if TARGET_DATE:
        log_detail(f"\nâš ï¸âš ï¸âš ï¸ æ³¨æ„ï¼šå½“å‰å¤„äºã€å›æµ‹æ¨¡å¼ã€‘ï¼Œç›®æ ‡æ—¥æœŸï¼š{TARGET_DATE} âš ï¸âš ï¸âš ï¸")
        log_detail("æœ¬æ¬¡è¿è¡Œå°†ã€ä¸ä¼šã€‘æ›´æ–° Panel å’Œ History JSON æ–‡ä»¶ã€‚")
    
    # 1. åŠ è½½é…ç½®å’Œæ˜ å°„
    tag_blacklist, hot_tags = load_tag_settings(TAGS_SETTING_JSON_FILE)
    symbol_to_sector_map = load_all_symbols(SECTORS_JSON_FILE, CONFIG["TARGET_SECTORS"])
    symbol_to_tags_map = load_symbol_tags(DESCRIPTION_JSON_FILE)

    if not symbol_to_sector_map:
        log_detail("é”™è¯¯: æ— æ³•åŠ è½½æ¿å—æ˜ å°„ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    # ================= ç­–ç•¥ 1 æ‰§è¡Œ =================
    # ä¼ å…¥ TARGET_DATEï¼Œå†…éƒ¨ä¼šå¤„ç†
    raw_pe_volume = process_condition_8(
        DB_FILE, 
        EARNING_HISTORY_JSON_FILE, 
        symbol_to_sector_map, 
        TARGET_DATE, 
        SYMBOL_TO_TRACE, 
        log_detail
    )
    final_pe_volume = sorted(list(set(raw_pe_volume)))

    # ================= ç­–ç•¥ 2 æ‰§è¡Œ =================
    # ä¼ å…¥ TARGET_DATEï¼Œå†…éƒ¨ä¼šå¤„ç†
    raw_pe_volume_up = process_pe_volume_up(
        DB_FILE,
        EARNING_HISTORY_JSON_FILE,
        symbol_to_sector_map,
        TARGET_DATE, 
        SYMBOL_TO_TRACE,
        log_detail
    )
    # final_pe_volume_up = sorted(list(set(raw_pe_volume_up)))

    # === æ–°å¢é€»è¾‘ï¼šå»é‡å¤„ç† ï¼Œå¦‚æœè¦æ¢å¤ä»£ç ï¼Œåªéœ€åˆ é™¤ä¸‹é¢æ¢å¤ä¸Šé¢å³å¯===
    # å¦‚æœ PE_Volume å·²ç»æœ‰äº†ï¼ŒPE_Volume_up å°±ä¸å†è¾“å‡º
    pe_volume_set = set(final_pe_volume)
    unique_pe_volume_up = set(raw_pe_volume_up)
    
    filtered_pe_volume_up = []
    
    log_detail("\n--- å¼€å§‹æ‰§è¡Œäº¤å‰å»é‡ (PE_Volume ä¼˜å…ˆ) ---")
    for sym in sorted(list(unique_pe_volume_up)):
        if sym in pe_volume_set:
            log_detail(f"    [å»é‡] Symbol {sym} å·²å­˜åœ¨äº PE_Volumeï¼Œä» PE_Volume_up ä¸­ç§»é™¤ã€‚")
        else:
            filtered_pe_volume_up.append(sym)
            
    final_pe_volume_up = sorted(filtered_pe_volume_up)
    log_detail(f"å»é‡å PE_Volume_up å‰©ä½™: {len(final_pe_volume_up)} ä¸ªã€‚")
    # ===========================

    # 4. æ„å»ºå¤‡æ³¨ (Note)
    def build_symbol_note_map(symbols):
        note_map = {}
        for sym in symbols:
            tags = set(symbol_to_tags_map.get(sym, []))
            is_hot = bool(tags & hot_tags)
            note_map[sym] = f"{sym}çƒ­" if is_hot else ""
        return note_map
        
    pe_volume_notes = build_symbol_note_map(final_pe_volume)
    pe_volume_up_notes = build_symbol_note_map(final_pe_volume_up)

    # 5. å›æµ‹å®‰å…¨æ‹¦æˆª
    # ã€å›æµ‹é€»è¾‘ã€‘å¦‚æœè®¾ç½®äº† TARGET_DATEï¼Œåœ¨è¿™é‡Œç›´æ¥ returnï¼Œä¸æ‰§è¡Œä¸‹é¢çš„å†™å…¥æ“ä½œ
    if TARGET_DATE:
        log_detail("\n" + "="*60)
        log_detail(f"ğŸ›‘ [å®‰å…¨æ‹¦æˆª] å›æµ‹æ¨¡å¼ (Date: {TARGET_DATE}) å·²å¯ç”¨ã€‚")
        log_detail(f"ğŸ“Š [ç­–ç•¥1] PE_Volume å‘½ä¸­: {len(final_pe_volume)} ä¸ª -> {final_pe_volume}")
        log_detail(f"ğŸ“Š [ç­–ç•¥2] PE_Volume_up å‘½ä¸­: {len(final_pe_volume_up)} ä¸ª -> {final_pe_volume_up}")
        log_detail("="*60 + "\n")
        return

    # 6. å†™å…¥ Panel
    log_detail(f"\næ­£åœ¨å†™å…¥ Panel æ–‡ä»¶...")
    # ç­–ç•¥1 å†™å…¥
    update_json_panel(final_pe_volume, PANEL_JSON_FILE, 'PE_Volume', symbol_to_note=pe_volume_notes)
    update_json_panel(final_pe_volume, PANEL_JSON_FILE, 'PE_Volume_backup', symbol_to_note=pe_volume_notes)
    
    # ç­–ç•¥2 å†™å…¥
    update_json_panel(final_pe_volume_up, PANEL_JSON_FILE, 'PE_Volume_up', symbol_to_note=pe_volume_up_notes)
    update_json_panel(final_pe_volume_up, PANEL_JSON_FILE, 'PE_Volume_up_backup', symbol_to_note=pe_volume_up_notes)

    # 7. å†™å…¥ History (Raw Data)
    log_detail(f"æ­£åœ¨æ›´æ–° History æ–‡ä»¶...")
    update_earning_history_json(EARNING_HISTORY_JSON_FILE, "PE_Volume", final_pe_volume, log_detail, base_date_str)
    update_earning_history_json(EARNING_HISTORY_JSON_FILE, "PE_Volume_up", final_pe_volume_up, log_detail, base_date_str)

    log_detail("ç¨‹åºè¿è¡Œç»“æŸã€‚")

def main():
    if SYMBOL_TO_TRACE:
        print(f"è¿½è¸ªæ¨¡å¼å·²å¯ç”¨ï¼Œç›®æ ‡: {SYMBOL_TO_TRACE}ã€‚æ—¥å¿—å°†å†™å…¥: {LOG_FILE_PATH}")
        try:
            with open(LOG_FILE_PATH, 'w', encoding='utf-8') as log_file:
                def log_detail_file(message):
                    log_file.write(message + '\n')
                    print(message)
                run_pe_volume_logic(log_detail_file)
        except IOError as e:
            print(f"é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æˆ–å†™å…¥æ—¥å¿—æ–‡ä»¶ {LOG_FILE_PATH}: {e}")
    else:
        print("è¿½è¸ªæ¨¡å¼æœªå¯ç”¨ã€‚æ—¥å¿—ä»…è¾“å‡ºåˆ°æ§åˆ¶å°ã€‚")
        def log_detail_console(message):
            print(message)
        run_pe_volume_logic(log_detail_console)

if __name__ == '__main__':
    main()