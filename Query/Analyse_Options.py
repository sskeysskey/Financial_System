import pandas as pd
import numpy as np
import os
import datetime
import glob
import subprocess
import json
import sqlite3
import sys
from datetime import timedelta
from pandas.tseries.holiday import USFederalHolidayCalendar

# ==========================================
# å…¨å±€é…ç½®åŒºåŸŸ (Configuration)
# ==========================================

USER_HOME = os.path.expanduser("~")
BASE_CODING_DIR = os.path.join(USER_HOME, "Coding")

# --- è·¯å¾„é…ç½® ---

# å¤‡ä»½æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„ (è‡ªåŠ¨æ¨¡å¼ç”¨)
BACKUP_DIR = os.path.join(BASE_CODING_DIR, "News", "backup")

# è¾“å‡ºæ–‡ä»¶çš„é…ç½® (a.py è¾“å‡º)
OUTPUT_DIR = os.path.join(BASE_CODING_DIR, "News")
OUTPUT_FILENAME = 'Options_Change.csv'

# ã€ä¿®æ”¹ã€‘æ–‡ä»¶åæ”¹ä¸º Historyï¼Œæš—ç¤ºè¿™æ˜¯ä¸€ä¸ªç´¯åŠ çš„æ–‡ä»¶
LARGE_PRICE_FILENAME = 'Options_History.csv' 

# JSON æ˜ å°„æ–‡ä»¶è·¯å¾„
SECTORS_JSON_PATH = os.path.join(BASE_CODING_DIR, "Financial_System", "Modules", "Sectors_All.json")

# SQLite æ•°æ®åº“è·¯å¾„ (å…±ç”¨)
DB_PATH = os.path.join(BASE_CODING_DIR, "Database", "Finance.db")
TABLE_NAME = 'Options'

# è°ƒè¯•è¾“å‡ºè·¯å¾„ (b.pyé€»è¾‘ç”¨)
OUTPUT_DEBUG_PATH = os.path.join(USER_HOME, "Downloads", "3.txt")

# --- ç®—æ³•å‚æ•°é…ç½® ---

# ã€ç­–ç•¥ 1ï¼šæ¯ä¸ª Symbol çš„ Calls å’Œ Puts å„ä¿ç•™å‰å¤šå°‘å (ç”¨äº Part A è¿‡æ»¤å’Œ Part B ç­–ç•¥1)ã€‘
TOP_N = 20

# [ç­–ç•¥ 2 (IV è®¡ç®—) å‚æ•°é…ç½®]
IV_TOP_N = 20           # å–æ’åå‰å¤šå°‘å
IV_DIVISOR = 7.0        # æœ€ç»ˆæ±‡æ€»æ—¶çš„é™¤æ•° (åŸä¸º 7)
IV_THRESHOLD = 20.0     # è·ç¦»é˜ˆå€¼ç™¾åˆ†æ¯” (åŸä¸º 20ï¼Œå³ 20%)
IV_ADJUSTMENT = 3.0     # è¶…è¿‡é˜ˆå€¼åçš„æƒé‡é™¤æ•° (åŸä¸º 3)

# [ç­–ç•¥ 3] é‡‘é¢é˜ˆå€¼ï¼Œé»˜è®¤1000ä¸‡ (10,000,000)
LARGE_PRICE_THRESHOLD = 10000000 

# a.py é€»è¾‘å‚æ•°: æ˜¯å¦è€ƒè™‘æ–°å¢çš„æ•°æ® (Bæœ‰Aæ— )
INCLUDE_NEW_ROWS = True

# ç­–ç•¥1ï¼š é€»è¾‘å‚æ•°: æƒé‡å¹‚æ¬¡é…ç½® (1=çº¿æ€§, 2=å¹³æ–¹...)
WEIGHT_POWER = 1

# b.py è°ƒè¯• Symbol
DEBUG_SYMBOL = ""

# --- æ¨¡å¼åˆ‡æ¢é…ç½® ---

# True: æ‰‹åŠ¨æ¨¡å¼ (ä½¿ç”¨ä¸‹æ–¹æŒ‡å®šçš„ä¸¤ä¸ªå…·ä½“æ–‡ä»¶)
# False: è‡ªåŠ¨æ¨¡å¼ (è‡ªåŠ¨å¯»æ‰¾ BACKUP_DIR ä¸‹æœ€æ–°çš„ä¸¤ä¸ªæ–‡ä»¶)
USE_MANUAL_MODE = False

# æ‰‹åŠ¨æ¨¡å¼ä¸‹çš„æ–‡ä»¶è·¯å¾„
MANUAL_FILE_OLD = os.path.join(BACKUP_DIR, 'Options_251224.csv')
MANUAL_FILE_NEW = os.path.join(BACKUP_DIR, 'Options_251227.csv')

# ==========================================
# [Part A] è¾…åŠ©å‡½æ•°ä¸æ ¸å¿ƒå¤„ç† (åŸ a.py)
# ==========================================

def load_symbol_sector_map(json_path):
    """åŠ è½½ JSON å¹¶åè½¬ä¸º Symbol -> Sector çš„å­—å…¸"""
    if not os.path.exists(json_path):
        print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° JSON æ˜ å°„æ–‡ä»¶: {json_path}")
        return {}
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        symbol_map = {}
        for sector, symbols in data.items():
            for sym in symbols:
                symbol_map[sym.upper()] = sector
        return symbol_map
    except Exception as e:
        print(f"âš ï¸ è¯»å– JSON å¤±è´¥: {e}")
        return {}

def get_latest_prices(symbols, symbol_sector_map, db_path):
    """
    æ‰¹é‡è·å– Symbol åœ¨ç³»ç»Ÿæ—¥æœŸä¹‹å‰ï¼ˆä¸å«ä»Šå¤©ï¼‰çš„æœ€æ–°ä»·æ ¼ã€‚
    å¦‚æœæ˜¨å¤©æ²¡æ•°æ®ï¼Œè‡ªåŠ¨å‘å‰è¿½æº¯ã€‚
    """
    if not os.path.exists(db_path):
        print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶: {db_path}")
        return {}

    price_dict = {}
    conn = None
    
    # è·å–ç³»ç»Ÿä»Šå¤©çš„æ—¥æœŸå­—ç¬¦ä¸² (æ ¼å¼: YYYY-MM-DD)
    today_str = datetime.datetime.now().strftime('%Y-%m-%d')
    
    try:
        conn = sqlite3.connect(db_path, timeout=60.0)
        cursor = conn.cursor()

        # æŒ‰æ¿å—åˆ†ç»„æŸ¥è¯¢ï¼Œæé«˜æ•ˆç‡
        sector_groups = {}
        for sym in symbols:
            sym_upper = sym.upper()
            lookup_sym = 'VIX' if sym_upper == '^VIX' else sym_upper
            sector = symbol_sector_map.get(lookup_sym)
            
            if sector:
                if sector not in sector_groups:
                    sector_groups[sector] = []
                sector_groups[sector].append(lookup_sym)

        print(f"æ­£åœ¨ä»æ•°æ®åº“è·å– {len(symbols)} ä¸ª Symbol åœ¨ {today_str} ä¹‹å‰çš„æœ€æ–°ä»·æ ¼...")

        for sector, sym_list in sector_groups.items():
            if not sym_list:
                continue

            # æ„å»ºå ä½ç¬¦
            placeholders = ','.join(['?'] * len(sym_list))
            
            # ä¿®æ”¹åçš„ SQL é€»è¾‘ï¼š
            # 1. å¢åŠ  WHERE date < ? ç¡®ä¿ä¸å–ä»Šå¤©çš„æ•°æ®
            # 2. é€šè¿‡ MAX(date) è‡ªåŠ¨è·å–è·ç¦»ä»Šå¤©æœ€è¿‘çš„é‚£ä¸ªå†å²æ—¥æœŸ
            query = f"""
                SELECT t1.name, t1.price
                FROM "{sector}" t1
                JOIN (
                    SELECT name, MAX(date) as max_date
                    FROM "{sector}"
                    WHERE name IN ({placeholders}) AND date < ?
                    GROUP BY name
                ) t2 ON t1.name = t2.name AND t1.date = t2.max_date
            """
            
            try:
                # å°† sym_list å’Œ today_str ä¼ å…¥æ‰§è¡Œ
                params = sym_list + [today_str]
                cursor.execute(query, params)
                rows = cursor.fetchall()
                
                for name, price in rows:
                    name_upper = name.upper()
                    price_dict[name_upper] = price
                    if name_upper == 'VIX':
                        price_dict['^VIX'] = price
            except Exception as e:
                print(f" âš ï¸ æŸ¥è¯¢è¡¨ '{sector}' å‡ºé”™: {e}")

    except Exception as e:
        print(f"æ•°æ®åº“è¿æ¥æˆ–æŸ¥è¯¢æ€»é”™è¯¯: {e}")
    finally:
        if conn:
            conn.close()
            
    return price_dict

def process_options_change(file_old, file_new, top_n=50, include_new=True):
    """
    å¤„ç†æœŸæƒå˜åŒ–é€»è¾‘ã€‚
    ä¿®æ”¹ï¼šç¡®ä¿ Price > 1000ä¸‡çš„æ•°æ®åœ¨ Top_N è¿‡æ»¤å‰è¢«æå–ã€‚
    """
    print(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] å¼€å§‹å¤„ç†æ–‡ä»¶æ¯”å¯¹...")
    print(f"æ—§æ–‡ä»¶: {os.path.basename(file_old)}")
    print(f"æ–°æ–‡ä»¶: {os.path.basename(file_new)}")

    if not os.path.exists(file_old) or not os.path.exists(file_new):
        print("é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ã€‚")
        return None

    try:
        # 1. åœ¨è¯»å–æ—¶åŒ…å« Last Price
        dtype_dict = {'Symbol': str, 'Expiry Date': str, 'Type': str, 'Strike': str, 'Last Price': str}
        df_old = pd.read_csv(file_old, dtype=dtype_dict)
        df_new = pd.read_csv(file_new, dtype=dtype_dict)
    except Exception as e:
        print(f"è¯»å–é”™è¯¯: {e}")
        return None

    # æ•°æ®æ¸…æ´—
    df_old.columns = df_old.columns.str.strip()
    df_new.columns = df_new.columns.str.strip()
    
    # --- æ¨èçš„æ¸…æ´—æ–¹å¼ ---
    def clean_numeric(val):
        if pd.isna(val): return 0.0
        if isinstance(val, (int, float)): return float(val)
        try: return float(str(val).replace(',', '').strip())
        except: return 0.0

    # ç»Ÿä¸€æ¸…æ´—ä¸¤ä¸ªæ–‡ä»¶çš„æ•°å€¼åˆ—
    for df_temp in [df_old, df_new]:
        # æ¸…æ´— Open Interest
        df_temp['Open Interest'] = df_temp.get('Open Interest', pd.Series(0)).apply(clean_numeric)
        
        # --- ã€ä¿®å¤ä»£ç å¼€å§‹ã€‘ ---
        # å¼ºåˆ¶ç¡®ä¿ 'Last Price' å­—æ®µå­˜åœ¨ã€‚
        # å¦‚æœæ—§æ–‡ä»¶ç¼ºå°‘æ­¤å­—æ®µï¼Œè¡¥ 0.0ï¼Œè¿™æ · merge æ—¶æ‰ä¼šäº§ç”Ÿ Last Price_old å’Œ Last Price_new
        if 'Last Price' not in df_temp.columns:
            df_temp['Last Price'] = 0.0
        else:
            df_temp['Last Price'] = df_temp['Last Price'].apply(clean_numeric)

    # è¿‡æ»¤å…¨æ–°æ—¥æœŸ
    print("æ­£åœ¨è¿‡æ»¤å…¨æ–°å‡ºç°çš„ Expiry Date ...")
    valid_old_dates = set(zip(df_old['Symbol'], df_old['Expiry Date']))
    df_new['_date_key'] = list(zip(df_new['Symbol'], df_new['Expiry Date']))
    rows_before = len(df_new)
    df_new = df_new[df_new['_date_key'].isin(valid_old_dates)].copy()
    print(f"å·²å‰”é™¤ {rows_before - len(df_new)} è¡Œå…¨æ–°æ—¥æœŸæ•°æ®ã€‚")
    df_new.drop(columns=['_date_key'], inplace=True)

    old_expiry_set = set(zip(df_old['Symbol'], df_old['Expiry Date']))
    
    # åˆå¹¶ (Last Price ä¼šå˜æˆ Last Price_new)
    key_columns = ['Symbol', 'Expiry Date', 'Type', 'Strike']
    merged = pd.merge(df_old, df_new, on=key_columns, how='outer', suffixes=('_old', '_new'), indicator=True)
    
    # è¿‡æ»¤é€»è¾‘
    merged = merged[merged['_merge'] != 'left_only'].copy()
    if not include_new:
        merged = merged[merged['_merge'] == 'both'].copy()
        
    merged['Open Interest_old'] = merged['Open Interest_old'].fillna(0)
    merged['Open Interest_new'] = merged['Open Interest_new'].fillna(0)
    merged['Last Price_new'] = merged['Last Price_new'].fillna(0)
    
    # å‰”é™¤æ—§æŒä»“ä¸º0çš„
    merged = merged[merged['Open Interest_old'] != 0].copy()
    
    # è®¡ç®— 1-Day Chg å’Œ Price
    merged['1-Day Chg'] = merged['Open Interest_new'] - merged['Open Interest_old']
    merged = merged[merged['1-Day Chg'] >= 0].copy()

    # --- ã€æ–°å¢æ­¥éª¤ã€‘è®¡ç®— Price åˆ— ---
    # å…¬å¼ï¼š1-Day Chg * Last Price (æ¥è‡ªæœ€æ–°æ–‡ä»¶)
    merged['Price'] = merged['1-Day Chg'] * merged['Last Price_new']

    # --- ã€ç­–ç•¥ 3ï¼šå¤§é¢å¼‚åŠ¨è¿½è¸ªé€»è¾‘æå‰ã€‘ ---
    # åœ¨è¿™é‡Œï¼Œmerged åŒ…å«æ‰€æœ‰å˜åŠ¨å¤§äº0çš„è¡Œï¼Œå°šæœªè¿›è¡Œ TOP_N è¿‡æ»¤
    
    large_price_raw = merged[merged['Price'] > LARGE_PRICE_THRESHOLD].copy()
    if not large_price_raw.empty:
        # ä¸ºå¤§é¢æ•°æ®å‡†å¤‡ Distance
        unique_l_symbols = large_price_raw['Symbol'].unique().tolist()
        symbol_map_l = load_symbol_sector_map(SECTORS_JSON_PATH)
        price_map_l = get_latest_prices(unique_l_symbols, symbol_map_l, DB_PATH)

        def calc_dist_temp(row):
            sym = row['Symbol'].upper()
            try: strike_val = float(str(row['Strike']).replace(',', '').strip())
            except: return "N/A"
            p_val = price_map_l.get(sym)
            if p_val is None or p_val == 0: return "N/A"
            return f"{((strike_val - p_val) / p_val) * 100:.2f}%"

        large_price_raw['Distance'] = large_price_raw.apply(calc_dist_temp, axis=1)
        
        # æ•´ç†æ ¼å¼
        current_date_str = datetime.datetime.now().strftime('%Y-%m-%d')
        large_price_raw['Run_Date'] = current_date_str
        large_price_raw = large_price_raw.rename(columns={'Open Interest_new': 'Open Interest'})
        
        l_cols = ['Run_Date', 'Symbol', 'Type', 'Expiry Date', 'Strike', 'Distance', 'Open Interest', '1-Day Chg', 'Price']
        large_price_final = large_price_raw[l_cols].copy()
        large_price_final['Symbol'] = large_price_final['Symbol'].replace('^VIX', 'VIX')

        # å†™å…¥å†å²åº“æ–‡ä»¶
        large_price_path = os.path.join(OUTPUT_DIR, LARGE_PRICE_FILENAME)
        if os.path.exists(large_price_path):
            try:
                history_df = pd.read_csv(large_price_path)
                if 'Run_Date' in history_df.columns:
                    history_clean = history_df[history_df['Run_Date'] != current_date_str]
                    final_save_df = pd.concat([history_clean, large_price_final], ignore_index=True)
                else:
                    final_save_df = pd.concat([history_df, large_price_final], ignore_index=True)
                final_save_df.to_csv(large_price_path, index=False)
                print(f"ğŸ”¥ å¤§é¢å˜åŠ¨å†å²åº“å·²æ›´æ–° (å…¨é‡ç›‘æ§): {large_price_path} (ä»Šæ—¥: {len(large_price_final)} æ¡)")
            except Exception as e:
                print(f"âš ï¸ å†å²åº“å†™å…¥å¤±è´¥: {e}")
        else:
            large_price_final.to_csv(large_price_path, index=False)
            print(f"ğŸ”¥ å¤§é¢å˜åŠ¨å†å²åº“å·²åˆ›å»º: {large_price_path}")

    # --- ã€å›åˆ°åŸæœ‰é€»è¾‘ï¼šæ‰§è¡Œ TOP_N è¿‡æ»¤ç”¨äºä¸»è¡¨å’Œè¯„åˆ†ã€‘ ---
    
    # æ ‡è®° new (ä»…é’ˆå¯¹å³å°†è¿›å…¥ Top N çš„æ•°æ®)
    if include_new and not merged.empty:
        def mark_new_rows(row):
            if row['_merge'] == 'right_only':
                if (row['Symbol'], row['Expiry Date']) not in old_expiry_set:
                    row['Expiry Date'] = str(row['Expiry Date']) + " new"
                else:
                    row['Strike'] = str(row['Strike']) + " new"
            return row
        merged = merged.apply(mark_new_rows, axis=1)

    # æ’åºå– Top N
    merged['Abs_Chg'] = merged['1-Day Chg'].abs()
    merged['Type_Rank'] = merged['Type'].str.lower().apply(lambda x: 0 if 'call' in x else 1)
    
    final_rows = []
    if not merged.empty:
        all_symbols = merged['Symbol'].unique()
        for symbol in all_symbols:
            symbol_df = merged[merged['Symbol'] == symbol]
            for type_val in symbol_df['Type'].unique():
                sub_df = symbol_df[symbol_df['Type'] == type_val]
                sub_df_sorted = sub_df.sort_values(by='Abs_Chg', ascending=False)
                final_rows.append(sub_df_sorted.head(top_n))

    if not final_rows:
        print("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®ã€‚")
        return None

    result_df = pd.concat(final_rows)

    # è®¡ç®— Distance
    print("æ­£åœ¨è®¡ç®— Distance ...")
    unique_symbols = result_df['Symbol'].unique().tolist()
    symbol_map = load_symbol_sector_map(SECTORS_JSON_PATH)
    price_map = get_latest_prices(unique_symbols, symbol_map, DB_PATH)

    def calculate_distance(row):
        sym = row['Symbol'].upper()
        strike_str = str(row['Strike']).replace(' new', '').strip()
        try: strike_val = float(strike_str.replace(',', ''))
        except: return "N/A"
        
        price_val = price_map.get(sym)
        if price_val is None: return "N/A"
        if price_val == 0: return "Err"
        
        dist = (strike_val - price_val) / price_val
        return f"{dist * 100:.2f}%"

    result_df['Distance'] = result_df.apply(calculate_distance, axis=1)

    # æœ€ç»ˆæ•´ç†
    result_df = result_df.sort_values(by=['Symbol', 'Type_Rank', 'Abs_Chg'], ascending=[True, True, False])
    
    # --- ã€ä¿®æ”¹ç‚¹ã€‘åœ¨è¾“å‡ºåˆ—ä¸­å¢åŠ  'Price' ---
    output_cols = ['Symbol', 'Type', 'Expiry Date', 'Strike', 'Distance', 'Open Interest_new', '1-Day Chg', 'Price']
    final_output = result_df[output_cols].rename(columns={'Open Interest_new': 'Open Interest'})
    final_output['Symbol'] = final_output['Symbol'].replace('^VIX', 'VIX')

    # ä¿å­˜æ–‡ä»¶
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    # 1. ä¿å­˜å¸¸è§„ä¸»æ–‡ä»¶ (è¿™ä¸ªæ–‡ä»¶é€šå¸¸è¿˜æ˜¯åªä¿ç•™å½“å¤©æœ€æ–°ï¼Œæˆ–è€…ä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹)
    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)
    final_output.to_csv(output_path, index=False)
    
    date_str = datetime.datetime.now().strftime('%y%m%d')
    backup_path = os.path.join(BACKUP_DIR, f"Options_Change_{date_str}.csv")
    if not os.path.exists(BACKUP_DIR): os.makedirs(BACKUP_DIR)
    final_output.to_csv(backup_path, index=False)
    print(f"âœ… å¤‡ä»½æ–‡ä»¶å·²ä¿å­˜: {backup_path}")

    # è¿”å› DataFrame ä¾›åç»­æ­¥éª¤ä½¿ç”¨
    return final_output

# ==========================================
# [Part B] è®¡ç®— D-Score åŠ IV å¹¶å…¥åº“
# ==========================================
# ä¿®æ”¹ç‚¹ï¼šå¢åŠ  iv_divisor, iv_threshold, iv_adj_factor å‚æ•°

def calculate_d_score_from_df(df_input, db_path, debug_path, n_config, iv_n_config, power_config, target_symbol, 
                              iv_divisor, iv_threshold, iv_adj_factor):
    """
    ç›´æ¥ä» DataFrame è®¡ç®— Score å¹¶å†™å…¥æ•°æ®åº“
    iv_n_config: ç­–ç•¥2å–æ’åçš„æ•°é‡
    iv_divisor: ç­–ç•¥2æœ€ç»ˆé™¤æ•°
    iv_threshold: ç­–ç•¥2è·ç¦»é˜ˆå€¼
    iv_adj_factor: ç­–ç•¥2æƒé‡è°ƒèŠ‚ç³»æ•°
    """
    print(f"\n[{datetime.datetime.now().strftime('%H:%M:%S')}] å¼€å§‹æ‰§è¡Œ Score ä¸ IV è®¡ç®—ä¸å…¥åº“...")
    print(f"å½“å‰é…ç½®: D-Score Top N = {n_config}, IV Top N = {iv_n_config}, æƒé‡å¹‚æ¬¡ = {power_config}")
    print(f"IVé…ç½®: é™¤æ•°={iv_divisor}, é˜ˆå€¼={iv_threshold}%, è°ƒèŠ‚ç³»æ•°={iv_adj_factor}")

    # è¿™é‡Œçš„ df_input å·²ç»æ˜¯å†…å­˜ä¸­çš„ DataFrameï¼Œé˜²æ­¢ä¿®æ”¹åŸæ•°æ®
    df = df_input.copy()

    # åˆå§‹åŒ–è°ƒè¯•æ–‡ä»¶
    if target_symbol:
        try:
            with open(debug_path, 'w') as f:
                f.write(f"=== {target_symbol} è®¡ç®—è¿‡ç¨‹è¿½è¸ªæ—¥å¿— ===\n")
                f.write(f"è¿è¡Œæ—¶é—´: {pd.Timestamp.now()}\n")
                f.write(f"æƒé‡å¹‚æ¬¡ (Power): {power_config}\n")
                f.write(f"IVå‚æ•°: Divisor={iv_divisor}, Threshold={iv_threshold}%, Adj={iv_adj_factor}\n\n")
        except: pass

    # --- æ•°æ®é¢„å¤„ç† (å…¼å®¹ a.py ç”Ÿæˆçš„æ ¼å¼) ---
    # 1. Distance å»ç™¾åˆ†å·ï¼Œè½¬ä¸ºå°æ•°
    try:
        df['Distance'] = df['Distance'].astype(str).str.rstrip('%').astype(float) / 100
    except:
        pass

    # 2. ç¡®ä¿æ•°å€¼åˆ—æ ¼å¼æ­£ç¡®
    for col in ['Open Interest', '1-Day Chg']:
        if df[col].dtype == object:
            df[col] = df[col].astype(str).str.replace(',', '').str.replace('%', '')
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    # 3. è§£ææ—¥æœŸ
    df['Expiry Date'] = pd.to_datetime(df['Expiry Date'], errors='coerce')
    if df['Expiry Date'].isnull().any():
        print("è­¦å‘Š: æœ‰éƒ¨åˆ†æ—¥æœŸæ— æ³•è§£æï¼Œå°†è¢«å¿½ç•¥ã€‚")
        df = df.dropna(subset=['Expiry Date'])

    # å‡†å¤‡å·¥ä½œæ—¥
    us_cal = USFederalHolidayCalendar()
    holidays = us_cal.holidays(start='2024-01-01', end='2030-12-31')
    today = pd.Timestamp.now().normalize()
    
    # å­˜å‚¨å¤„ç†ç»“æœ
    processed_data = {}
    grouped = df.groupby(['Symbol', 'Type'])

    print(f"å¼€å§‹è®¡ç®—åˆ†æ•°... (è°ƒè¯•ç›®æ ‡: {target_symbol})")

    for (symbol, type_), group in grouped:
        # ç¡®ä¿è¯¥ Symbol åœ¨å­—å…¸ä¸­åˆå§‹åŒ–
        if symbol not in processed_data:
            processed_data[symbol] = {'Call': 0.0, 'Put': 0.0, 'Call_IV_Sum': 0.0, 'Put_IV_Sum': 0.0}
        
        # æŒ‰æ•°å€¼é™åºæ’åˆ—
        group = group.sort_values(by='1-Day Chg', ascending=False)
        
        # ---------------------------
        # ç­–ç•¥ 1: D-Score é€»è¾‘ (å·²ä¿®æ”¹: åŸºäº 1-Day Chg åŠ æƒ)
        # ---------------------------
        top_items = group.head(n_config).copy()
        D = 0 
        
        # è°ƒè¯•æ•°æ®å®¹å™¨
        strat1_debug_rows = []
        
        if not top_items.empty:
            max_expiry = top_items['Expiry Date'].max()
            
            # è®¡ç®— A (æ—¥æœŸè·ç¦»)
            a_days = np.busday_count(
                np.array([today], dtype='datetime64[D]'),
                np.array([max_expiry], dtype='datetime64[D]'),
                holidays=holidays.values.astype('datetime64[D]')
            )[0]
            
            # è®¡ç®—æ¯è¡Œå·®å€¼
            expiry_dates = top_items['Expiry Date'].values.astype('datetime64[D]')
            today_arr = np.full(expiry_dates.shape, today).astype('datetime64[D]')
            days_i = np.busday_count(today_arr, expiry_dates, holidays=holidays.values.astype('datetime64[D]'))
            
            diff_i = a_days - days_i
            diff_pow = diff_i ** power_config
            B_val = np.sum(diff_pow) # æ”¹åé˜²æ­¢å˜é‡æ··æ·†
            
            # [ä¿®æ”¹ç‚¹ 1] è®¡ç®—æ€»çš„ 1-Day Chg ä½œä¸ºåˆ†æ¯ï¼Œè€Œé Total Open Interest
            total_chg = top_items['1-Day Chg'].sum()
            C_val = 0
            
            # [ä¿®æ”¹ç‚¹ 2] åˆ¤æ–­æ¡ä»¶æ”¹ä¸º total_chg != 0
            if B_val != 0 and total_chg != 0:
                w_i = diff_pow / B_val
                
                # [ä¿®æ”¹ç‚¹ 3] æ ¸å¿ƒå…¬å¼ä¿®æ”¹ï¼š
                # åŸæ¥: ... * top_items['Open Interest'].values
                # ç°åœ¨: ... * top_items['1-Day Chg'].values
                scores = w_i * top_items['Distance'].values * top_items['1-Day Chg'].values
                C_val = np.sum(scores)
                
                # --- ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ã€‘ ---
                # ç»Ÿè®¡çœŸæ­£å¯¹ C æœ‰è´¡çŒ®çš„è¡Œæ•°ï¼š
                # 1. 1-Day Chg å¿…é¡»å¤§äº 0
                # 2. diff_i å¿…é¡»å¤§äº 0 (å³ä¸æ˜¯æœ€è¿œçš„é‚£ä¸€å¤©ï¼Œå› ä¸ºæœ€è¿œé‚£ä¸€å¤©çš„æƒé‡æ˜¯0)
                valid_mask = (top_items['1-Day Chg'] > 0) & (diff_i > 0)
                valid_count = np.sum(valid_mask)
                
                if total_chg > 0:
                    # ä½¿ç”¨æœ‰æ•ˆè¡Œæ•°ä½œä¸ºä¹˜æ•°ï¼Œè€Œä¸æ˜¯å›ºå®šçš„ n_config(20)
                    D = C_val * valid_count / total_chg
                # ----------------------
                
                if symbol == target_symbol:
                    for idx in range(len(top_items)):
                        strat1_debug_rows.append({
                            'Expiry': top_items.iloc[idx]['Expiry Date'].strftime('%Y-%m-%d'),
                            '1-Day Chg': top_items.iloc[idx]['1-Day Chg'],
                            'Dist': top_items.iloc[idx]['Distance'],
                            'Diff_i': diff_i[idx],
                            'Weight': w_i[idx],
                            'Score': scores[idx],
                            'IsValid': "Yes" if valid_mask[idx] else "No"
                        })

        # å­˜å…¥ D-Score
        type_str = str(type_).lower()
        if 'call' in type_str:
            processed_data[symbol]['Call'] = D
        elif 'put' in type_str:
            processed_data[symbol]['Put'] = D
            
        # ===========================
        # ç­–ç•¥ 2: æ–° IV é€»è¾‘ (ä½¿ç”¨ iv_n_config)
        # ===========================
        # ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ iv_n_config æ›¿æ¢ç¡¬ç¼–ç çš„ 10
        top_iv_items = group.head(iv_n_config).copy()
        iv_weighted_sum = 0.0
        total_chg_iv = top_iv_items['1-Day Chg'].sum()
        strat2_debug_rows = []
        
        for i in range(len(top_iv_items)):
            row_data = top_iv_items.iloc[i]
            dist_val = row_data['Distance'] * 100 
            chg_val = row_data['1-Day Chg']
            base_weight = chg_val / total_chg_iv if total_chg_iv != 0 else 0.0
            
            # [ä¿®æ”¹ç‚¹ 4] ä¿æŒåŸæœ‰è§„åˆ™: ç»å¯¹å€¼ >= 20% æ—¶ï¼Œç³»æ•°é™¤ä»¥ 3
            final_weight = base_weight
            is_adjusted = False
            # [ä¿®æ”¹ç‚¹] æƒé‡è§„åˆ™: ç»å¯¹å€¼ >= iv_threshold æ—¶ï¼Œç³»æ•°é™¤ä»¥ iv_adj_factor
            if abs(dist_val) >= iv_threshold:
                final_weight = base_weight / iv_adj_factor
                is_adjusted = True
            
            contribution = dist_val * final_weight
            iv_weighted_sum += contribution
            
            # æ”¶é›†ç­–ç•¥2è°ƒè¯•ä¿¡æ¯
            if symbol == target_symbol:
                strat2_debug_rows.append({
                    'Rank': i + 1,
                    'Expiry': row_data['Expiry Date'].strftime('%Y-%m-%d'),
                    'Strike': row_data['Strike'],
                    'Dist_Pct': dist_val,
                    '1-Day Chg': chg_val, # å¢åŠ  Chg æ˜¾ç¤ºæ–¹ä¾¿è°ƒè¯•æƒé‡æ¥æº
                    'Base_Wt': base_weight,
                    'Adj?': "YES" if is_adjusted else "No",
                    'Final_Wt': final_weight,
                    'Contrib': contribution
                })

        # å­˜å…¥ IV ä¸­é—´å€¼
        if 'call' in type_str:
            processed_data[symbol]['Call_IV_Sum'] = iv_weighted_sum
        elif 'put' in type_str:
            processed_data[symbol]['Put_IV_Sum'] = iv_weighted_sum

        # ---------------------------
        # ç»Ÿä¸€å†™å…¥è°ƒè¯•æ–‡ä»¶ (ç­–ç•¥1 + ç­–ç•¥2)
        # ---------------------------
        if symbol == target_symbol:
            log_lines = [f"\n{'='*80}\næ­£åœ¨è®¡ç®—: {symbol} - {type_}"]
            log_lines.append(f"\n[Strategy 1 - D-Score] (Top {n_config})")
            log_lines.append(f"A={a_days}, B={B_val:.4f}, C={C_val:.6f}")
            log_lines.append(f"æœ‰æ•ˆè¡Œæ•°(Chg>0ä¸”Diff>0)={valid_count}, Final D={D:.6f}")
            
            header1 = f"{'Expiry':<12} | {'Diff_i':<6} | {'Weight':<10} | {'Dist':<8} | {'Chg':<8} | {'Valid?':<6} | {'Score'}"
            log_lines.append(header1 + "\n" + "-"*len(header1))
            for r in strat1_debug_rows:
                log_lines.append(f"{r['Expiry']:<12} | {r['Diff_i']:<6} | {r['Weight']:.6f} | {r['Dist']:.4f} | {r['1-Day Chg']:<8.0f} | {r['IsValid']:<6} | {r['Score']:.6f}")

            log_lines.append(f"\n[Strategy 2 - IV] (Top {iv_n_config})")
            header2 = f"{'Rank':<4} | {'Expiry':<12} | {'Dist(%)':<8} | {'Chg':<8} | {'FinalWt':<8} | {'Contrib'}"
            log_lines.append(header2 + "\n" + "-"*len(header2))
            for r in strat2_debug_rows:
                log_lines.append(f"{r['Rank']:<4} | {r['Expiry']:<12} | {r['Dist_Pct']:>7.2f}% | {r['1-Day Chg']:<8.0f} | {r['Final_Wt']:.4f} | {r['Contrib']:.4f}")
            
            with open(debug_path, 'a') as f: f.write('\n'.join(log_lines) + '\n')

    # --- æ•°æ®åº“å†™å…¥é€»è¾‘ ---
    print(f"æ­£åœ¨è¿æ¥æ•°æ®åº“: {db_path} ...")
    
    # è®¾å®šå†™å…¥æ—¥æœŸ
    target_date = (pd.Timestamp.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    print(f"å†™å…¥æ—¥æœŸè®¾å®šä¸º: {target_date}")
    
    conn = sqlite3.connect(db_path, timeout=60.0)
    cursor = conn.cursor()

    # 1. å»ºè¡¨
    # æ³¨æ„: IV å­—æ®µç±»å‹ç°åœ¨å»ºè®®ä¸º TEXT ä»¥å­˜å‚¨ç™¾åˆ†æ¯”å­—ç¬¦ä¸²
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        date TEXT,
        name TEXT,
        call TEXT,
        put TEXT,
        price REAL,
        change REAL,
        iv TEXT,
        UNIQUE(date, name)
    )
    """
    cursor.execute(create_table_sql)

    # 2. æ£€æŸ¥å¹¶è‡ªåŠ¨æ·»åŠ åˆ—
    # å¦‚æœåˆ—å·²å­˜åœ¨ (å³ä½¿æ˜¯ REAL ç±»å‹)ï¼ŒSQLite ä¹Ÿå…è®¸å­˜å…¥å­—ç¬¦ä¸²ï¼Œæ‰€ä»¥è¿™é‡Œé€»è¾‘å…¼å®¹æ€§å¾ˆé«˜
    for col_name, col_type in [('change', 'REAL'), ('iv', 'TEXT')]:
        try:
            cursor.execute(f"SELECT {col_name} FROM {TABLE_NAME} LIMIT 1")
        except:
            print(f"æ£€æµ‹åˆ°ç¼ºå°‘ '{col_name}' åˆ—ï¼Œæ­£åœ¨æ·»åŠ ...")
            try:
                cursor.execute(f"ALTER TABLE {TABLE_NAME} ADD COLUMN {col_name} {col_type}")
            except Exception as e:
                print(f"æ·»åŠ  {col_name} åˆ—å¤±è´¥: {e}")

    query_prev_price_sql = f"SELECT price FROM {TABLE_NAME} WHERE name = ? AND date < ? ORDER BY date DESC LIMIT 1"
    
    insert_sql = f"""
    INSERT INTO {TABLE_NAME} (date, name, call, put, price, change, iv)
    VALUES (?, ?, ?, ?, ?, ?, ?)
    ON CONFLICT(date, name) DO UPDATE SET
        call=excluded.call,
        put=excluded.put,
        price=excluded.price,
        change=excluded.change,
        iv=excluded.iv
    """
    
    count_success = 0
    
    for symbol, values in processed_data.items():
        # --- ç­–ç•¥ 1 ç»“æœ ---
        raw_call_d = values['Call']
        raw_put_d = values['Put']
        call_str = f"{raw_call_d * 100:.2f}%"
        put_str = f"{raw_put_d * 100:.2f}%"
        final_price = round((raw_call_d + raw_put_d) * 100, 2)
        
        # è®¡ç®— Change
        change_val = None
        try:
            cursor.execute(query_prev_price_sql, (symbol, target_date))
            row = cursor.fetchone()
            if row and row[0] is not None:
                prev_price = row[0]
                change_val = round(final_price - prev_price, 2)
        except:
            change_val = None
            
        # --- ç­–ç•¥ 2 ç»“æœ ---
        # [ä¿®æ”¹ç‚¹] ä½¿ç”¨é…ç½®çš„ iv_divisor
        sum_a = values['Call_IV_Sum']
        sum_b = values['Put_IV_Sum']
        raw_iv_val = (sum_a + sum_b) / iv_divisor
        
        # [ä¿®æ”¹] æ ¼å¼åŒ–ä¸ºç™¾åˆ†æ¯”å­—ç¬¦ä¸²ï¼Œä¿ç•™2ä½å°æ•°
        final_iv = f"{raw_iv_val:.2f}%"

        try:
            cursor.execute(insert_sql, (target_date, symbol, call_str, put_str, final_price, change_val, final_iv))
            count_success += 1
        except Exception as e:
            print(f"é”™è¯¯: å†™å…¥/æ›´æ–° {symbol} å¤±è´¥: {e}")

    conn.commit()
    conn.close()
    print(f"å…¥åº“å®Œæˆï¼å·²å¤„ç†ï¼ˆæ’å…¥æˆ–æ›´æ–°ï¼‰: {count_success} æ¡æ•°æ®")

# ==========================================
# å·¥å…·å‡½æ•° & Main
# ==========================================

def get_latest_two_files(directory, pattern='Options_*.csv'):
    """è‡ªåŠ¨è·å–æœ€æ–°çš„ä¸¤ä¸ªæ–‡ä»¶"""
    search_path = os.path.join(directory, pattern)
    files = glob.glob(search_path)
    
    # è¿‡æ»¤æ‰æ–‡ä»¶åä¸­åŒ…å« 'Change' æˆ– 'History' çš„å¤‡ä»½æ–‡ä»¶ï¼Œé˜²æ­¢è¯»å…¥ä¸Šæ¬¡çš„è¿è¡Œç»“æœ
    files = [f for f in files if 'Change' not in os.path.basename(f) and 'History' not in os.path.basename(f)]
    
    files.sort(reverse=True)
    
    # è°ƒè¯•æ‰“å°ï¼Œæ–¹ä¾¿ç¡®è®¤è¯»åˆ°äº†å“ªä¸¤ä¸ªæ–‡ä»¶
    if len(files) >= 2:
        print(f"DEBUG: è‡ªåŠ¨é€‰ä¸­æœ€æ–°æ–‡ä»¶ (New): {os.path.basename(files[0])}")
        print(f"DEBUG: è‡ªåŠ¨é€‰ä¸­æ¬¡æ–°æ–‡ä»¶ (Old): {os.path.basename(files[1])}")
        
    if len(files) < 2: return None, None
    return files[0], files[1]

def show_alert(message):
    try:
        if sys.platform == 'darwin':
            applescript_code = f'display dialog "{message}" buttons {{"OK"}} default button "OK"'
            subprocess.run(['osascript', '-e', applescript_code], check=True)
        elif sys.platform == 'win32':
            import ctypes
            ctypes.windll.user32.MessageBoxW(0, message, "æç¤º", 0)
    except Exception:
        pass

if __name__ == "__main__":
    file_new = None
    file_old = None

    # 1. ç¡®å®šæ–‡ä»¶è·¯å¾„
    if USE_MANUAL_MODE:
        print(">>> æ¨¡å¼: æ‰‹åŠ¨æŒ‡å®š (Manual Mode)")
        file_old = MANUAL_FILE_OLD
        file_new = MANUAL_FILE_NEW
        if not (os.path.exists(file_old) and os.path.exists(file_new)):
            print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æŒ‡å®šçš„æ‰‹åŠ¨æ–‡ä»¶ã€‚")
            file_new = None
    else:
        print(">>> æ¨¡å¼: è‡ªåŠ¨æ‰«æ (Auto Mode)")
        file_new, file_old = get_latest_two_files(BACKUP_DIR)
        if not file_new:
            print("âŒ é”™è¯¯: å¤‡ä»½ç›®å½•ä¸‹æ–‡ä»¶ä¸è¶³ä¸¤ä¸ªã€‚")

    # 2. å¼€å§‹æ‰§è¡Œæµç¨‹
    if file_new and file_old:
        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†å¹¶ç”Ÿæˆ Change æ•°æ®
        generated_df = process_options_change(file_old, file_new, TOP_N, INCLUDE_NEW_ROWS)
        
        # ç¬¬äºŒæ­¥ï¼šå¦‚æœç”ŸæˆæˆåŠŸï¼Œç›´æ¥åœ¨å†…å­˜ä¸­ä¼ é€’æ•°æ®è¿›è¡Œå…¥åº“è®¡ç®—
        if generated_df is not None and not generated_df.empty:
            # ä¿®æ”¹ç‚¹ï¼šåœ¨è°ƒç”¨æ—¶ä¼ å…¥äº†æ–°å¢çš„ IV å‚æ•°
            calculate_d_score_from_df(
                generated_df, 
                DB_PATH, 
                OUTPUT_DEBUG_PATH, 
                TOP_N, 
                IV_TOP_N, 
                WEIGHT_POWER, 
                DEBUG_SYMBOL,
                IV_DIVISOR,     # æ–°å¢
                IV_THRESHOLD,   # æ–°å¢
                IV_ADJUSTMENT   # æ–°å¢
            )
            show_alert("æµç¨‹å®Œæˆï¼šCSVå·²ç”Ÿæˆï¼Œæ•°æ®åº“å·²æ›´æ–°")
        else:
            print("\nâš ï¸ æœªç”Ÿæˆæœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡æ•°æ®åº“è®¡ç®—æ­¥éª¤ã€‚")
    else:
        print("\nç¨‹åºç»ˆæ­¢: æœªèƒ½è·å–æœ‰æ•ˆçš„å¯¹æ¯”æ–‡ä»¶ã€‚")
