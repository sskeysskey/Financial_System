import json
import os
from datetime import datetime, timedelta
import shutil

# --- é…ç½®éƒ¨åˆ† ---

USER_HOME = os.path.expanduser("~")

# å®šä¹‰æºæ–‡ä»¶å’Œç›®æ ‡ç›®å½•çš„è·¯å¾„
LOCAL_DOWNLOAD_BACKUP = os.path.join(USER_HOME, 'Downloads/backup/DB_backup')

# æ–°å¢çš„å¸¦æ—¶é—´æˆ³å¤‡ä»½çš„ç›®æ ‡ç›®å½•
LOCAL_SERVER_DIR = os.path.join(USER_HOME, 'Coding/LocalServer/Resources/Finance')

# version.json æ–‡ä»¶è·¯å¾„
VERSION_JSON_PATH = os.path.join(LOCAL_SERVER_DIR, 'version.json')

# å®šä¹‰éœ€è¦è¿›è¡Œç®€å•è¦†ç›–å¤‡ä»½çš„æ–‡ä»¶
# æ ¼å¼ä¸º: { "æºæ–‡ä»¶è·¯å¾„": ["ç›®æ ‡æ–‡ä»¶è·¯å¾„1", "ç›®æ ‡æ–‡ä»¶è·¯å¾„2", ...] }
SIMPLE_BACKUP_FILES = {
    os.path.join(USER_HOME, 'Coding/Database/Finance.db'): [
        os.path.join(LOCAL_DOWNLOAD_BACKUP, 'Finance.db'),
        os.path.join(LOCAL_SERVER_DIR,    'Finance.db')
    ],
}

# å®šä¹‰éœ€è¦è¿›è¡Œæ—¶é—´æˆ³å¤‡ä»½çš„æºæ–‡ä»¶åˆ—è¡¨
TIMESTAMP_BACKUP_SOURCES = [
    os.path.join(USER_HOME, 'Coding/News/backup/Compare_All.txt'),
    os.path.join(USER_HOME, 'Coding/News/Earnings_Release_new.txt'),
    os.path.join(USER_HOME, 'Coding/News/Earnings_Release_next.txt'),
    os.path.join(USER_HOME, 'Coding/News/Earnings_Release_third.txt'),
    os.path.join(USER_HOME, 'Coding/News/Earnings_Release_fourth.txt'),
    os.path.join(USER_HOME, 'Coding/News/Earnings_Release_fifth.txt'),
    os.path.join(USER_HOME, 'Coding/News/HighLow.txt'),
    os.path.join(USER_HOME, 'Coding/Financial_System/Modules/tags_weight.json'),
    os.path.join(USER_HOME, 'Coding/Financial_System/Modules/Sectors_panel.json'), # è§¦å‘ Intro_Symbol
    os.path.join(USER_HOME, 'Coding/Financial_System/Modules/Sectors_All.json'),
    os.path.join(USER_HOME, 'Coding/Financial_System/Modules/description.json'),
    os.path.join(USER_HOME, 'Coding/News/CompareStock.txt'),
    os.path.join(USER_HOME, 'Coding/News/CompareETFs.txt'),
    os.path.join(USER_HOME, 'Coding/News/10Y_newhigh_stock.txt'), # è§¦å‘ Intro_Symbol
    os.path.join(USER_HOME, 'Coding/News/Options_Change.csv'),
    os.path.join(USER_HOME, 'Coding/News/Options_History.csv'),
    os.path.join(USER_HOME, 'Coding/Financial_System/Modules/Earning_History.json'),
]

# å®šä¹‰è§¦å‘ Intro_Symbol æ›´æ–°çš„ç‰¹å®šæ–‡ä»¶é›†åˆ
INTRO_SYMBOL_TRIGGERS = {
    os.path.join(USER_HOME, 'Coding/Financial_System/Modules/Sectors_panel.json'),
    os.path.join(USER_HOME, 'Coding/News/10Y_newhigh_stock.txt'),
    os.path.join(USER_HOME, 'Coding/News/Options_History.csv')
}

def is_file_modified(source_path, destination_path):
    """
    æ¨¡æ‹Ÿ rsync é€»è¾‘ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–ã€‚
    å¦‚æœè¿”å› Trueï¼Œè¯´æ˜éœ€è¦å¤åˆ¶ï¼›è¿”å› Falseï¼Œè¯´æ˜æ–‡ä»¶ä¸€è‡´ï¼Œè·³è¿‡ã€‚
    åˆ¤æ–­ä¾æ®ï¼š
    1. ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ -> éœ€è¦å¤åˆ¶
    2. æ–‡ä»¶å¤§å°ä¸åŒ -> éœ€è¦å¤åˆ¶
    3. ä¿®æ”¹æ—¶é—´ (mtime) ä¸åŒ -> éœ€è¦å¤åˆ¶
    """
    if not os.path.exists(destination_path):
        return True

    try:
        s_stat = os.stat(source_path)
        d_stat = os.stat(destination_path)

        # 1. æ£€æŸ¥å¤§å° (Size)
        if s_stat.st_size != d_stat.st_size:
            return True

        # 2. æ£€æŸ¥ä¿®æ”¹æ—¶é—´ (Mtime)
        # æ³¨æ„ï¼šæ–‡ä»¶ç³»ç»Ÿä¹‹é—´çš„æ—¶é—´æˆ³ç²¾åº¦å¯èƒ½ä¸åŒï¼Œè¿™é‡Œå…è®¸ 1 ç§’ä»¥å†…çš„è¯¯å·®ï¼Œæˆ–è€…ä½ å¯ä»¥ä¸¥æ ¼ä½¿ç”¨ !=
        # rsync é»˜è®¤é€»è¾‘æ˜¯: size å¿…é¡»ä¸åŒ æˆ–è€… mtime å¿…é¡»ä¸åŒ
        if int(s_stat.st_mtime) != int(d_stat.st_mtime):
            return True

        return False

    except OSError:
        # å¦‚æœè·å–å±æ€§å¤±è´¥ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œè®¤ä¸ºéœ€è¦å¤åˆ¶
        return True

def smart_copy(source_path, destination_path):
    """
    æ™ºèƒ½å¤åˆ¶ï¼šä»…åœ¨æºæ–‡ä»¶æœ‰å˜åŒ–æ—¶æ‰è¦†ç›–ã€‚
    è¿”å› True è¡¨ç¤ºè¿›è¡Œäº†å¤åˆ¶ï¼ŒFalse è¡¨ç¤ºè·³è¿‡ã€‚
    """
    try:
        # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(source_path):
            print(f"è­¦å‘Šï¼šæºæ–‡ä»¶æœªæ‰¾åˆ° {source_path}")
            return False

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤åˆ¶
        if is_file_modified(source_path, destination_path):
            os.makedirs(os.path.dirname(destination_path), exist_ok=True)
            # copy2 ä¼šåŒæ—¶ä¿ç•™æ–‡ä»¶çš„å…ƒæ•°æ®ï¼ˆåŒ…æ‹¬ mtimeï¼‰ï¼Œè¿™å¯¹äºä¸‹æ¬¡æ¯”å¯¹è‡³å…³é‡è¦
            shutil.copy2(source_path, destination_path)
            print(f"âœ… [æ›´æ–°] å·²å¤åˆ¶: {os.path.basename(source_path)} -> {destination_path}")
            return True # è¿”å› True è¡¨ç¤ºæ–‡ä»¶å‘ç”Ÿäº†å˜åŒ–
        else:
            print(f"â­ï¸ [è·³è¿‡] æ— å˜åŒ–: {os.path.basename(source_path)}")
            return False # è¿”å› False è¡¨ç¤ºæ–‡ä»¶æœªå˜åŒ–
    except Exception as e:
        print(f"âŒ å¤åˆ¶æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def backup_with_timestamp_and_cleanup():
    """
    æ‰§è¡Œå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½ï¼Œæ¸…ç†æ—§æ–‡ä»¶ï¼Œå¹¶æ›´æ–°version.jsonã€‚
    """
    print("\n--- å¼€å§‹æ‰§è¡Œæ—¶é—´æˆ³å¤‡ä»½å’Œæ¸…ç†ä»»åŠ¡ ---")
    
    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    os.makedirs(LOCAL_SERVER_DIR, exist_ok=True)
    
    # 1. è®¡ç®—æ—¶é—´æˆ³
    yesterday = datetime.now() - timedelta(days=1)
    timestamp = yesterday.strftime('%y%m%d')  # æ ¼å¼åŒ–ä¸º YYMMDD

    newly_created_files_info = []
    source_base_names = set()
    
    # æ–°å¢ï¼šç”¨äºè®°å½•æœ¬æ¬¡å®é™…æ›´æ–°äº†å“ªäº›æºæ–‡ä»¶
    updated_source_paths = []

    # 2. å¤åˆ¶æ–‡ä»¶å¹¶æ·»åŠ æ—¶é—´æˆ³
    for source_path in TIMESTAMP_BACKUP_SOURCES:
        if not os.path.exists(source_path):
            print(f"è­¦å‘Š: æºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½: {source_path}")
            continue

        # åˆ†ç¦»æ–‡ä»¶åå’Œæ‰©å±•å
        base_name, extension = os.path.splitext(os.path.basename(source_path))
        source_base_names.add(base_name) # è®°å½•åŸºç¡€æ–‡ä»¶åç”¨äºåç»­æ¸…ç†
        
        # æ„å»ºæ–°çš„å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åå’Œç›®æ ‡è·¯å¾„
        new_filename = f"{base_name}_{timestamp}{extension}"
        destination_path = os.path.join(LOCAL_SERVER_DIR, new_filename)
        
        # æ‰§è¡Œæ™ºèƒ½å¤åˆ¶ï¼Œå¹¶æ•è·è¿”å›å€¼
        is_updated = smart_copy(source_path, destination_path)
        
        # å¦‚æœæ–‡ä»¶å‘ç”Ÿäº†å®è´¨æ€§æ›´æ–°ï¼ˆå¤åˆ¶æ“ä½œï¼‰ï¼Œè®°å½•ä¸‹æ¥
        if is_updated:
            updated_source_paths.append(source_path)
        
        file_type = 'text' if extension.lower() == '.txt' else extension.lstrip('.').lower()
        newly_created_files_info.append({
            "name": new_filename,
            "type": file_type
        })

    # 3. æ¸…ç†æ—§æ–‡ä»¶
    print("\n--- å¼€å§‹æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶ ---")
    for filename in os.listdir(LOCAL_SERVER_DIR):
        # åˆ†ç¦»æ–‡ä»¶åï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬ç®¡ç†çš„æ–‡ä»¶
        parts = filename.split('_')
        if len(parts) > 1:
            base_name = '_'.join(parts[:-1])
            file_timestamp_ext = parts[-1]
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å±äºæœ¬æ¬¡å¤‡ä»½çš„ç±»å‹ï¼Œå¹¶ä¸”æ—¶é—´æˆ³ä¸æ˜¯æœ€æ–°çš„
            if base_name in source_base_names and not file_timestamp_ext.startswith(timestamp):
                file_to_delete = os.path.join(LOCAL_SERVER_DIR, filename)
                try:
                    os.remove(file_to_delete)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ–‡ä»¶: {file_to_delete}")
                except OSError as e:
                    print(f"åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™ {file_to_delete}: {e}")

    # 4. æ›´æ–° version.json
    print("\n--- å¼€å§‹æ›´æ–° version.json ---")
    # å°†æœ¬æ¬¡å®é™…æ›´æ–°çš„æ–‡ä»¶åˆ—è¡¨ä¼ é€’ç»™ update å‡½æ•°
    update_version_json(newly_created_files_info, source_base_names, updated_source_paths)

def update_version_json(new_files_info, updated_base_names, updated_files_list):
    """
    æ›´æ–°version.jsonæ–‡ä»¶ï¼šå¢åŠ ç‰ˆæœ¬å·ï¼Œå¤„ç† Eco_Data å’Œ Intro_Symbol æ—¶é—´æˆ³ã€‚
    """
    try:
        # è¯»å–ç°æœ‰çš„version.jsonï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„ç»“æ„
        if os.path.exists(VERSION_JSON_PATH):
            with open(VERSION_JSON_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            print(f"æœªæ‰¾åˆ° {VERSION_JSON_PATH}ï¼Œå°†åˆ›å»ºä¸€ä¸ªæ–°çš„ã€‚")
            data = {"version": "1.0", "files": []}

        # --- æ ¸å¿ƒé€»è¾‘ä¿®æ”¹ï¼šæ›´æ–° Eco_Data å’Œ Intro_Symbol ---
        current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M')
        
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰ Intro_Symbol è§¦å‘æ–‡ä»¶å‘ç”Ÿäº†å˜æ›´
        # å¦‚æœ updated_files_list ä¸­åŒ…å« INTRO_SYMBOL_TRIGGERS ä¸­çš„ä»»æ„ä¸€ä¸ª
        intro_updated = False
        for trigger_file in INTRO_SYMBOL_TRIGGERS:
            if trigger_file in updated_files_list:
                intro_updated = True
                break
        
        if intro_updated:
            data['Intro_Symbol'] = current_time_str
            print(f"â° æ£€æµ‹åˆ°å…³é”®æ–‡ä»¶å˜æ›´ï¼Œå·²æ›´æ–° Intro_Symbol: {current_time_str}")

        # 2. æ£€æŸ¥æ˜¯å¦æœ‰â€œå…¶ä»–æ–‡ä»¶â€å‘ç”Ÿäº†å˜æ›´ (Eco_Data)
        # é€»è¾‘ï¼šå˜æ›´åˆ—è¡¨ä¸­å­˜åœ¨ ä¸å±äº INTRO_SYMBOL_TRIGGERS çš„æ–‡ä»¶
        eco_updated = False
        for updated_file in updated_files_list:
            if updated_file not in INTRO_SYMBOL_TRIGGERS:
                eco_updated = True
                break
        
        if eco_updated:
            data['Eco_Data'] = current_time_str
            print(f"â° æ£€æµ‹åˆ°å¸¸è§„æ–‡ä»¶å˜æ›´ï¼Œå·²æ›´æ–° Eco_Data: {current_time_str}")
        
        # ç¡®ä¿è¿™ä¸¤ä¸ªå­—æ®µå­˜åœ¨ï¼ˆå³ä½¿æ²¡æœ‰æ›´æ–°ï¼Œä¹Ÿä¿è¯jsonç»“æ„å®Œæ•´ï¼Œå¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼‰
        if 'Intro_Symbol' not in data:
            data['Intro_Symbol'] = current_time_str
        if 'Eco_Data' not in data:
            data['Eco_Data'] = current_time_str

        # -----------------------------------------------

        # æ›´æ–°ç‰ˆæœ¬å·
        try:
            major, minor = data.get('version', '1.0').split('.')
            data['version'] = f"{major}.{int(minor) + 1}"
            print(f"ç‰ˆæœ¬å·å·²æ›´æ–°ä¸º: {data['version']}")
        except ValueError:
            print("è­¦å‘Š: ç‰ˆæœ¬å·æ ¼å¼ä¸æ­£ç¡®ï¼Œé‡ç½®ä¸º '1.1'")
            data['version'] = '1.1'

        # è¿‡æ»¤æ—§æ–‡ä»¶æ¡ç›®å¹¶æ·»åŠ æ–°æ¡ç›®
        existing_files = data.get('files', [])
        filtered_files = []
        
        for entry in existing_files:
            name = entry.get('name', '')
            parts = name.split('_')
            if len(parts) > 1:
                base_name = '_'.join(parts[:-1])
                # åªæœ‰å½“è¿™ä¸ªåŸºç¡€åä¸åœ¨æœ¬æ¬¡å¤„ç†åˆ—è¡¨ä¸­æ—¶ï¼Œæ‰ä¿ç•™ï¼ˆå› ä¸ºæ–°çš„ä¼šéšåæ·»åŠ ï¼‰
                if base_name not in updated_base_names:
                    filtered_files.append(entry)
            else:
                filtered_files.append(entry)

        data['files'] = filtered_files + new_files_info

        with open(VERSION_JSON_PATH, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)

        print(f"{VERSION_JSON_PATH} å·²æˆåŠŸæ›´æ–°ã€‚")

    except json.JSONDecodeError:
        print(f"é”™è¯¯: {VERSION_JSON_PATH} æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œæ— æ³•è§£æã€‚")
    except Exception as e:
        print(f"æ›´æ–° {VERSION_JSON_PATH} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# --- ä¸»ç¨‹åºæ‰§è¡Œ ---

if __name__ == "__main__":
    
    # 1. æ‰§è¡Œç®€å•çš„è¦†ç›–å¤‡ä»½
    print("--- å¼€å§‹æ‰§è¡Œç®€å•è¦†ç›–å¤‡ä»½ ---")
    for source, dest_list in SIMPLE_BACKUP_FILES.items():
        for dest in dest_list:
            smart_copy(source, dest)

    # 2. æ‰§è¡Œå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½ã€æ¸…ç†å’Œversion.jsonæ›´æ–°
    backup_with_timestamp_and_cleanup()
    
    print("\næ‰€æœ‰ä»»åŠ¡å·²å®Œæˆã€‚")
